{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City of LA Parcel Tax × Zoning Analysis\n",
    "\n",
    "## Project Overview\n",
    "This project analyzes property tax efficiency patterns across zoning categories and proximity to transit (based on SB 79) in the City of Los Angeles. By combining spatial data, zoning information, and property tax data, we examine how different land use designations contribute to municipal revenue and model potential impacts of transit-oriented rezoning scenarios. The analysis provides data-driven insights to support policy decisions related to land use optimization near major transit stops.\n",
    "\n",
    "## Data Sources\n",
    "* City of LA parcel geometries (`Parcels.gdb`)\n",
    "* Transit stop proximity tiers based on SB79 (`Metro_Stops_SB_79.geojson`)\n",
    "* Simplified zoning category mappings (`Parcel tax by zoning - assigned zoning.csv`)\n",
    "* Property tax data for all LA parcels (`Parcel_Data_2021_Table_-2691831558175163259.csv`)\n",
    "* City zoning designations (`ZONING_PLY_20250403.geojson`)\n",
    "\n",
    "## Methodology\n",
    "* Processing and integrating parcel geometries with tax assessment data\n",
    "* Categorizing parcels by simplified zoning designations\n",
    "* Calculating tax revenue per acre across different zoning types\n",
    "* Analyzing parcels by transit proximity tiers per SB79 definitions\n",
    "* Modeling financial impacts of potential rezoning scenarios near transit\n",
    "* Building a regression model training pipeline to attempt to find accurate ways to predict SB 79 up-zoning property tax increases\n",
    "\n",
    "## Key Inputs\n",
    "* Parcel-level property tax assessments (2024 snapshot)\n",
    "* Parcel acreage and geometry\n",
    "* Zoning designations and simplified category mappings\n",
    "* Transit stop locations and proximity tiers\n",
    "* Current land use patterns\n",
    "\n",
    "## Outputs\n",
    "* Tax efficiency metrics by zoning category\n",
    "* Scenario models for potential rezoning impacts\n",
    "* Visualizations of geographic patterns in tax productivity\n",
    "\n",
    "## Notes\n",
    "This analysis focuses on the City of Los Angeles only and uses a zoning dataset that intentionally excludes recent DTLA rezoning changes for consistency purposes. The transit proximity tiers are defined according to SB79 parameters. This analysis is the input to a full [report](https://data.streetsforall.org/blog/sb79_zoning_budget/) on the data.streetsforall.org blog, and should be read alongside that written piece for full context and nuance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rtree\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from shapely import Point\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Scikit-learn modules for models and evaluation metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVR\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property Tax, Zoning, and Parcels Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in both CSV files\n",
    "file1_path = \"data/Parcel_Data_2021_Table_-2691831558175163259.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "merged_df = pd.read_csv(file1_path)\n",
    "\n",
    "# Display basic info about the dataframes\n",
    "print(f\"Dataset 1 shape: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df[\"City Tax Rate Area\"] == \"LOS ANGELES\"]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = merged_df[\n",
    "    [\n",
    "        \"Zip Code\",\n",
    "        \"City Tax Rate Area\",\n",
    "        \"AIN\",\n",
    "        \"Roll Year\",\n",
    "        \"Tax Rate Area Code\",\n",
    "        \"Property Location\",\n",
    "        \"Property Use Type\",\n",
    "        \"Property Use Code\",\n",
    "        \"Number of Buildings\",\n",
    "        \"Year Built\",\n",
    "        \"Effective Year\",\n",
    "        \"Square Footage\",\n",
    "        \"Number of Bedrooms\",\n",
    "        \"Number of Bathrooms\",\n",
    "        \"Number of Units\",\n",
    "        \"Land Value\",\n",
    "        \"Land Base Year\",\n",
    "        \"Improvement Value\",\n",
    "        \"Improvement Base Year\",\n",
    "        \"Total Value, Land + Improvement\",\n",
    "        \"Home Owners Exemption\",\n",
    "        \"Real Estate Exemption\",\n",
    "        \"Fixture Value\",\n",
    "        \"Fixture Exemption\",\n",
    "        \"Personal Property Value\",\n",
    "        \"Personal Property Exemption\",\n",
    "        \"Property taxable?\",\n",
    "        \"Total Value\",\n",
    "        \"Total Exemption\",\n",
    "        \"Taxable Value\",\n",
    "        \"Classification\",\n",
    "        \"Region Number\",\n",
    "        \"Location Latitude\",\n",
    "        \"Location Longitude\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Geocode Zoning Data\n",
    "Extract relevant information from the zoning designation and map to parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoning = gpd.read_file(\"data/ZONING_PLY_20250403.geojson\")\n",
    "zoning = zoning[[\"zone_cmplt\", \"geometry\"]]\n",
    "zoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert the regular dataframe to a GeoDataFrame by creating Point geometries\n",
    "# First, make a copy to avoid modifying the original\n",
    "selected_gdf = selected.copy()\n",
    "\n",
    "# Create a geometry column with Point objects from latitude and longitude\n",
    "selected_gdf[\"geometry\"] = selected_gdf.apply(\n",
    "    lambda row: Point(row[\"Location Longitude\"], row[\"Location Latitude\"]), axis=1\n",
    ")\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "selected_gdf = gpd.GeoDataFrame(selected_gdf, geometry=\"geometry\")\n",
    "\n",
    "# Make sure both GeoDataFrames have the same CRS (Coordinate Reference System)\n",
    "# If you know the CRS of your data, set it explicitly\n",
    "# For example, if your coordinates are in WGS84:\n",
    "selected_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# Ensure zoning has the same CRS, or reproject if needed\n",
    "if zoning.crs != selected_gdf.crs:\n",
    "    zoning = zoning.to_crs(selected_gdf.crs)\n",
    "\n",
    "# Step 2: Perform spatial join - this will add zoning attributes to each point\n",
    "joined_data = gpd.sjoin(selected_gdf, zoning, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 3: If you want a regular dataframe with the original columns plus zoning info\n",
    "# (You might want to drop the extra geometry column and index_right column)\n",
    "result = pd.DataFrame(joined_data.drop(columns=[\"geometry\", \"index_right\"]))\n",
    "\n",
    "# Alternatively, if you want to add specific columns from zoning to your original dataframe:\n",
    "# selected['zoning_type'] = joined_data['zoning_type']  # Replace with your actual column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"ZONE_PREFIX\"] = result[\"zone_cmplt\"].str.split(\"-\").str[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_file(\"data/Parcels.gdb\", columns=[\"AIN\", \"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels[\"AIN\"] = pd.to_numeric(parcels[\"AIN\"], errors=\"coerce\").astype(\n",
    "    \"Int64\"\n",
    ")  # Keeps NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(\n",
    "    result,\n",
    "    parcels,\n",
    "    on=\"AIN\",  # Join on the AIN column that exists in both dataframes\n",
    "    how=\"left\",  # Keep all rows from 'result' even if no matching AIN in parcels\n",
    ")\n",
    "\n",
    "# If you want to convert the merged result back to a GeoDataFrame\n",
    "# (in case you need to do more spatial operations later)\n",
    "# Note: This will use the geometry from the parcels dataframe\n",
    "merged_gdf = gpd.GeoDataFrame(merged_results, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Acreage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_gdf = merged_gdf.to_crs(\"EPSG:2229\")\n",
    "\n",
    "# Calculate area in square feet first (State Plane uses US Survey Feet)\n",
    "projected_gdf[\"area_sq_feet\"] = projected_gdf.geometry.area\n",
    "\n",
    "# Convert to acres (1 acre = 43,560 square feet)\n",
    "projected_gdf[\"acreage\"] = projected_gdf[\"area_sq_feet\"] / 43560\n",
    "\n",
    "# Drop the intermediate area calculation if you don't need it\n",
    "projected_gdf = projected_gdf.drop(columns=[\"area_sq_feet\"])\n",
    "\n",
    "# If you want to round the acreage to a specific number of decimal places\n",
    "projected_gdf[\"acreage\"] = projected_gdf[\"acreage\"].round(3)\n",
    "\n",
    "# Convert back to original CRS if needed for further spatial operations\n",
    "final_gdf = projected_gdf.to_crs(merged_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Prop 13 the property tax rate is 1%\n",
    "\n",
    "final_gdf[\"Property_Tax_Value\"] = final_gdf[\"Taxable Value\"] * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf = final_gdf.dropna(subset=[\"acreage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf[\"Property_Tax_Value\"].sum() * 0.2478\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract first letter of zoning code\n",
    "def extract_first_letter(zone_code):\n",
    "    if pd.isna(zone_code):\n",
    "        return None\n",
    "\n",
    "    # Remove any brackets or parentheses and their contents\n",
    "    # This regex looks for patterns like [...], (...), etc.\n",
    "    cleaned_code = re.sub(r\"[\\[\\(].*?[\\]\\)]\", \"\", zone_code)\n",
    "    cleaned_prefix = cleaned_code\n",
    "\n",
    "    # Remove any remaining special characters\n",
    "    cleaned_code = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_code)\n",
    "    cleaned_code = cleaned_code.replace(\"Q\", \"\")\n",
    "\n",
    "    # Take the first letter if there is one\n",
    "    if cleaned_code and len(cleaned_code) > 0:\n",
    "        return cleaned_code[0], cleaned_prefix\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def extract_zone_district_clean(zone_code):\n",
    "    if pd.isna(zone_code):\n",
    "        return None\n",
    "\n",
    "    # Remove anything inside () or [] and the brackets themselves\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", zone_code).strip()\n",
    "\n",
    "    cleaned = cleaned.replace(\"Q\", \"\")\n",
    "\n",
    "    # Find the first occurrence of a dash and keep one character after it\n",
    "    match = re.match(r\"([A-Za-z0-9]+-\\w?)\", cleaned)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Apply to final_gdf\n",
    "final_gdf[\"zone_district_clean\"] = final_gdf[\"zone_cmplt\"].apply(\n",
    "    extract_zone_district_clean\n",
    ")\n",
    "\n",
    "\n",
    "# Apply and split into two columns properly\n",
    "zone_extracted = final_gdf[\"ZONE_PREFIX\"].apply(extract_first_letter).apply(pd.Series)\n",
    "zone_extracted.columns = [\"zone_type\", \"zone_prefix_clean\"]\n",
    "\n",
    "# Add them to the original DataFrame\n",
    "final_gdf[[\"zone_type\", \"zone_prefix_clean\"]] = zone_extracted\n",
    "\n",
    "# Remove all 'Q' characters from zone_prefix_clean and zone_district_clean\n",
    "final_gdf[\"zone_prefix_clean\"] = final_gdf[\"zone_prefix_clean\"].str.replace(\n",
    "    \"Q\", \"\", regex=False\n",
    ")\n",
    "final_gdf[\"zone_district_clean\"] = final_gdf[\"zone_district_clean\"].str.replace(\n",
    "    \"Q\", \"\", regex=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logic for zone_consolidation\n",
    "def consolidate_zone(row):\n",
    "    if row[\"zone_prefix_clean\"] in [\"C2\", \"C4\"]:\n",
    "        return row[\"zone_district_clean\"]\n",
    "    return row[\"zone_prefix_clean\"]\n",
    "\n",
    "\n",
    "# Apply the function row-wise\n",
    "final_gdf[\"zone_consolidation\"] = final_gdf.apply(consolidate_zone, axis=1)\n",
    "print(final_gdf[\"zone_consolidation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_gdf[\"ZONE_PREFIX\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_gdf[\"zone_prefix_clean\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf[final_gdf[\"zone_consolidation\"] == None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_gdf[\"zone_consolidation\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SFA's zoning buckets and map to parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spreadsheet (update path as needed)\n",
    "zoning_map_df = pd.read_csv(\"data/Parcel tax by zoning - assigned zoning.csv\")\n",
    "\n",
    "# Create the dictionary mapping\n",
    "abridged_to_bucket = dict(\n",
    "    zip(zoning_map_df[\"Abridged Zoning Class\"], zoning_map_df[\"Assigned Zoning Bucket\"])\n",
    ")\n",
    "\n",
    "print(abridged_to_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the dictionary to create a new column\n",
    "final_gdf[\"assigned_zoning_bucket\"] = final_gdf[\"zone_consolidation\"].map(\n",
    "    abridged_to_bucket\n",
    ")\n",
    "final_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf[\"assigned_zoning_bucket\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = final_gdf[\"assigned_zoning_bucket\"].isna().sum()\n",
    "print(f\"Rows with missing zoning bucket: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_buckets = final_gdf[final_gdf[\"assigned_zoning_bucket\"] != \"Ignore\"]\n",
    "final_buckets = final_buckets.dropna(subset=[\"assigned_zoning_bucket\"])\n",
    "final_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_geom = final_buckets[\n",
    "    [\n",
    "        \"geometry\",\n",
    "        \"AIN\",\n",
    "        \"Property Location\",\n",
    "        \"zone_type\",\n",
    "        \"acreage\",\n",
    "        \"Number of Units\",\n",
    "        \"Property_Tax_Value\",\n",
    "        \"zone_cmplt\",\n",
    "        \"zone_consolidation\",\n",
    "        \"assigned_zoning_bucket\",\n",
    "        \"Location Latitude\",\n",
    "        \"Location Longitude\",\n",
    "        \"Taxable Value\",\n",
    "        \"Property taxable?\",\n",
    "    ]\n",
    "]\n",
    "final_geom.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect data for issues before beggining merge process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics on your tax column\n",
    "print(\"Tax value summary statistics:\")\n",
    "print(final_geom[\"Property_Tax_Value\"].describe())\n",
    "\n",
    "# Check the largest values - they might be outliers\n",
    "print(\"\\nTop 10 largest tax values:\")\n",
    "print(\n",
    "    final_geom.nlargest(10, \"Property_Tax_Value\")[\n",
    "        [\"AIN\", \"assigned_zoning_bucket\", \"Property_Tax_Value\", \"acreage\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Are there any negative or zero values?\n",
    "print(\n",
    "    f\"\\nCount of zero/negative tax values: {len(final_geom[final_geom['Property_Tax_Value'] <= 0])}\"\n",
    ")\n",
    "\n",
    "# Check spatial distribution - are all properties truly in LA City?\n",
    "if \"geometry\" in final_gdf.columns:\n",
    "    # Calculate area of study\n",
    "    bounds = final_geom.total_bounds\n",
    "    area_km2 = (\n",
    "        (bounds[2] - bounds[0]) * (bounds[3] - bounds[1]) / 1000000\n",
    "    )  # rough estimate\n",
    "    print(f\"\\nApproximate geographic area covered: {area_km2:.2f} km²\")\n",
    "\n",
    "    # LA City is roughly 1,300 km² - if your area is much larger, you have data outside the city\n",
    "\n",
    "# Calculate mean tax per property\n",
    "mean_tax = final_geom[\"Property_Tax_Value\"].mean()\n",
    "count = len(final_geom)\n",
    "print(f\"\\nMean tax per property: ${mean_tax:.2f}\")\n",
    "print(f\"Number of properties: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_geom[\"acreage\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_geom.to_file(\"final_geom.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Parcel Geospatial Data\n",
    "Properties like condos and gated communities feature duplicates geometry, throwing off acreage calculation so it is neccesary to merge overlapping parcels together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk processing function at module level so it can be pickled\n",
    "def _process_chunk(chunk_data):\n",
    "    \"\"\"\n",
    "    Process a chunk of polygons to find overlaps.\n",
    "\n",
    "    Parameters:\n",
    "    chunk_data: Tuple containing (chunk_idx, gdf, areas, chunk_size, n, overlap_threshold, debug)\n",
    "\n",
    "    Returns:\n",
    "    Tuple of (rows, cols) with indices of overlapping polygons\n",
    "    \"\"\"\n",
    "    chunk_idx, gdf, areas, chunk_size, n, overlap_threshold, debug = chunk_data\n",
    "\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, n)\n",
    "    chunk_indices = range(start_idx, end_idx)\n",
    "\n",
    "    # Create R-tree spatial index for this chunk\n",
    "    idx = rtree.index.Index()\n",
    "    for i, idx_val in enumerate(chunk_indices):\n",
    "        # Skip zero area geometries when building index\n",
    "        if areas[idx_val] <= 0:\n",
    "            continue\n",
    "        idx.insert(i, gdf.geometry.iloc[idx_val].bounds)\n",
    "\n",
    "    # Find overlaps\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    for i, idx1 in enumerate(chunk_indices):\n",
    "        # Skip zero area geometries\n",
    "        if areas[idx1] <= 0:\n",
    "            continue\n",
    "\n",
    "        geom1 = gdf.geometry.iloc[idx1]\n",
    "        area1 = areas[idx1]\n",
    "        bounds = geom1.bounds\n",
    "\n",
    "        # Find potential overlaps using spatial index\n",
    "        for j in idx.intersection(bounds):\n",
    "            idx2 = start_idx + j\n",
    "\n",
    "            # Only check forward to avoid duplicates\n",
    "            if idx2 <= idx1:\n",
    "                continue\n",
    "\n",
    "            # Skip zero area geometries\n",
    "            if areas[idx2] <= 0:\n",
    "                continue\n",
    "\n",
    "            # Get geometry\n",
    "            geom2 = gdf.geometry.iloc[idx2]\n",
    "\n",
    "            # Quick intersection check\n",
    "            if not geom1.intersects(geom2):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Calculate intersection\n",
    "                intersection = geom1.intersection(geom2)\n",
    "                intersection_area = intersection.area\n",
    "\n",
    "                # Calculate overlap ratios\n",
    "                ratio1 = intersection_area / area1\n",
    "                ratio2 = intersection_area / areas[idx2]\n",
    "\n",
    "                # Check if either ratio exceeds threshold\n",
    "                if ratio1 >= overlap_threshold or ratio2 >= overlap_threshold:\n",
    "                    rows.append(idx1)\n",
    "                    cols.append(idx2)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"Error checking overlap between {idx1} and {idx2}: {str(e)}\")\n",
    "\n",
    "    return rows, cols\n",
    "\n",
    "\n",
    "# Define component processing function at module level\n",
    "def _process_component(component_data):\n",
    "    \"\"\"\n",
    "    Process a component (group of overlapping polygons).\n",
    "    Instead of merging geometries, use geometry with highest acreage.\n",
    "\n",
    "    Parameters:\n",
    "    component_data: Tuple containing (component_id, gdf, labels)\n",
    "\n",
    "    Returns:\n",
    "    Tuple of (merged_data_dict, merge_count)\n",
    "    \"\"\"\n",
    "    component_id, gdf, labels = component_data\n",
    "\n",
    "    # Get indices for this component\n",
    "    component_indices = np.where(labels == component_id)[0]\n",
    "\n",
    "    if len(component_indices) == 1:\n",
    "        # Single property, no merging needed\n",
    "        return gdf.iloc[component_indices[0]].to_dict(), 0\n",
    "    else:\n",
    "        # Get the properties to merge\n",
    "        group_data = gdf.iloc[component_indices]\n",
    "\n",
    "        # Find index of property with highest acreage\n",
    "        if \"acreage\" in group_data.columns:\n",
    "            max_acreage_idx = group_data[\"acreage\"].idxmax()\n",
    "            # Use geometry of property with highest acreage\n",
    "            main_geom = group_data.loc[max_acreage_idx, \"geometry\"]\n",
    "        else:\n",
    "            # If no acreage column, use the largest geometry by area\n",
    "            max_area_idx = group_data[\"_area\"].idxmax()\n",
    "            main_geom = group_data.loc[max_area_idx, \"geometry\"]\n",
    "\n",
    "        # Prepare aggregated data\n",
    "        agg_data = {}\n",
    "\n",
    "        # For all numeric columns, sum them\n",
    "        for col in group_data.select_dtypes(include=np.number).columns:\n",
    "            if col not in [\"Location Latitude\", \"Location Longitude\", \"_area\"]:\n",
    "                agg_data[col] = group_data[col].sum()\n",
    "\n",
    "        # Keep the max acreage value\n",
    "        if \"acreage\" in group_data.columns:\n",
    "            agg_data[\"acreage\"] = group_data[\"acreage\"].max()\n",
    "\n",
    "        # For lat/long, take median values\n",
    "        if \"Location Latitude\" in group_data.columns:\n",
    "            agg_data[\"Location Latitude\"] = group_data[\"Location Latitude\"].median()\n",
    "        if \"Location Longitude\" in group_data.columns:\n",
    "            agg_data[\"Location Longitude\"] = group_data[\"Location Longitude\"].median()\n",
    "\n",
    "        # For string columns, join unique values\n",
    "        for col in group_data.select_dtypes(include=[\"object\"]).columns:\n",
    "            unique_values = group_data[col].dropna().astype(str).unique()\n",
    "            agg_data[col] = \", \".join(unique_values) if len(unique_values) > 0 else None\n",
    "\n",
    "        # For boolean columns, use logical OR\n",
    "        for col in group_data.select_dtypes(include=[\"bool\"]).columns:\n",
    "            agg_data[col] = any(group_data[col])\n",
    "\n",
    "        # Set the geometry to the one with highest acreage\n",
    "        agg_data[\"geometry\"] = main_geom\n",
    "\n",
    "        return agg_data, len(component_indices)\n",
    "\n",
    "\n",
    "def merge_overlapping_properties_serial(\n",
    "    gdf, overlap_threshold=0.8, chunk_size=10000, debug=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes chunks sequentially.\n",
    "    Uses geometry with highest acreage for each merged group.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame with property polygons\n",
    "    overlap_threshold (float): Minimum overlap ratio required for merging (0.0 to 1.0)\n",
    "    chunk_size (int): Number of polygons to process in each chunk\n",
    "    debug (bool): Whether to print debug messages\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: Processed GeoDataFrame with merged properties\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Starting memory-efficient property polygon merging...\")\n",
    "\n",
    "    try:\n",
    "        # Make a copy to avoid modifying the original\n",
    "        gdf = gdf.copy()\n",
    "        original_crs = gdf.crs\n",
    "\n",
    "        # Convert to UTM Zone 11N for accurate spatial calculations\n",
    "        projected_crs = \"EPSG:32611\"  # UTM Zone 11N (meters)\n",
    "        if debug:\n",
    "            print(f\"Converting from {original_crs} to {projected_crs} for processing\")\n",
    "\n",
    "        # Convert to projected CRS\n",
    "        gdf = gdf.to_crs(projected_crs)\n",
    "\n",
    "        # Fix any invalid geometries\n",
    "        if debug:\n",
    "            print(\"Fixing invalid geometries...\")\n",
    "        gdf[\"geometry\"] = gdf.geometry.apply(\n",
    "            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n",
    "        )\n",
    "\n",
    "        # Pre-compute areas for all polygons\n",
    "        if debug:\n",
    "            print(\"Pre-computing areas...\")\n",
    "        gdf[\"_area\"] = gdf.geometry.area\n",
    "        areas = gdf[\"_area\"].values\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"Area statistics: Min={min(areas):.2f}, Max={max(areas):.2f}, Mean={np.mean(areas):.2f}\"\n",
    "            )\n",
    "\n",
    "        # Calculate number of chunks\n",
    "        n = len(gdf)\n",
    "        n_chunks = math.ceil(n / chunk_size)\n",
    "        if debug:\n",
    "            print(f\"Processing {n} properties in {n_chunks} chunks of {chunk_size}...\")\n",
    "\n",
    "        # Process all chunks serially to save memory\n",
    "        all_rows = []\n",
    "        all_cols = []\n",
    "\n",
    "        for chunk_idx in tqdm(range(n_chunks), disable=not debug):\n",
    "            rows, cols = _process_chunk(\n",
    "                (chunk_idx, gdf, areas, chunk_size, n, overlap_threshold, debug)\n",
    "            )\n",
    "            all_rows.extend(rows)\n",
    "            all_cols.extend(cols)\n",
    "\n",
    "            # Free memory periodically\n",
    "            if chunk_idx % 10 == 0 and debug:\n",
    "                import gc\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "        # Check if any overlaps were found\n",
    "        if not all_rows:\n",
    "            if debug:\n",
    "                print(\"No overlapping properties detected with the current threshold.\")\n",
    "                print(\n",
    "                    f\"Try a lower overlap_threshold value (current: {overlap_threshold})\"\n",
    "                )\n",
    "            return gdf.drop(columns=[\"_area\"]).to_crs(original_crs)\n",
    "\n",
    "        # Create sparse matrix for the entire dataset\n",
    "        if debug:\n",
    "            print(\"Creating adjacency matrix...\")\n",
    "\n",
    "        # Add symmetric relationships (more efficiently)\n",
    "        all_rows.extend(all_cols)\n",
    "        all_cols.extend(all_rows[: -len(all_cols)])\n",
    "        data = np.ones(len(all_rows), dtype=np.int8)\n",
    "\n",
    "        adjacency_matrix = csr_matrix((data, (all_rows, all_cols)), shape=(n, n))\n",
    "\n",
    "        # Find connected components (groups of overlapping properties)\n",
    "        if debug:\n",
    "            print(\"Finding connected components...\")\n",
    "        n_components, labels = connected_components(adjacency_matrix, directed=False)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Found {n_components} distinct property groups\")\n",
    "\n",
    "        # Check if any merging actually happened\n",
    "        if n_components == n:\n",
    "            if debug:\n",
    "                print(\"No properties were merged.\")\n",
    "            return gdf.drop(columns=[\"_area\"]).to_crs(original_crs)\n",
    "\n",
    "        # Process components sequentially to save memory\n",
    "        new_rows = []\n",
    "        merged_count = 0\n",
    "\n",
    "        if debug:\n",
    "            print(\"Processing and merging components...\")\n",
    "\n",
    "        # Pre-allocate a list to store processed components\n",
    "        processed_components = []\n",
    "\n",
    "        for component_id in tqdm(range(n_components), disable=not debug):\n",
    "            row_dict, count = _process_component((component_id, gdf, labels))\n",
    "            processed_components.append(row_dict)\n",
    "            merged_count += count\n",
    "\n",
    "            # Free memory periodically\n",
    "            if component_id % 1000 == 0 and debug:\n",
    "                import gc\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "        # Create GeoDataFrame in one operation from processed components\n",
    "        if debug:\n",
    "            print(\"Creating final GeoDataFrame...\")\n",
    "        merged_gdf = gpd.GeoDataFrame(processed_components, crs=projected_crs)\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"Original properties: {len(gdf)}, Merged properties: {len(merged_gdf)}\"\n",
    "            )\n",
    "            print(f\"Properties merged into groups: {merged_count}\")\n",
    "\n",
    "            if \"Property_Tax_Value\" in merged_gdf.columns:\n",
    "                print(\n",
    "                    f\"Total tax before merging: ${gdf['Property_Tax_Value'].sum():,.2f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Total tax after merging: ${merged_gdf['Property_Tax_Value'].sum():,.2f}\"\n",
    "                )\n",
    "\n",
    "        # Convert back to original CRS\n",
    "        merged_gdf = merged_gdf.to_crs(original_crs)\n",
    "\n",
    "        # Drop internal calculation columns\n",
    "        if \"_area\" in merged_gdf.columns:\n",
    "            merged_gdf = merged_gdf.drop(columns=[\"_area\"])\n",
    "\n",
    "        return merged_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error during merging: {str(e)}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "        return gdf.drop(columns=[\"_area\"] if \"_area\" in gdf.columns else []).to_crs(\n",
    "            original_crs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = merge_overlapping_properties_serial(\n",
    "    final_geom, overlap_threshold=0.8, chunk_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"assigned_zoning_bucket\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing some cleanup of the merge process, as there are a few parcels merged across zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result[\"assigned_zoning_bucket\"].isin([\"R3, R2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result[\"assigned_zoning_bucket\"].isin([\"R1, R2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result[\"assigned_zoning_bucket\"].isin([\"R5, R4\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\n",
    "    result[\"assigned_zoning_bucket\"].isin([\"R3, R2\"]), \"assigned_zoning_bucket\"\n",
    "] = \"R3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\n",
    "    result[\"assigned_zoning_bucket\"].isin([\"R1, R2\"]), \"assigned_zoning_bucket\"\n",
    "] = \"R2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\n",
    "    result[\"assigned_zoning_bucket\"].isin([\"R5, R4\"]), \"assigned_zoning_bucket\"\n",
    "] = \"R5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\n",
    "    result[\"assigned_zoning_bucket\"].isin([\"R4, R2\"]), \"assigned_zoning_bucket\"\n",
    "] = \"R4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\n",
    "    result[\"assigned_zoning_bucket\"].isin([\"R4, R3\"]), \"assigned_zoning_bucket\"\n",
    "] = \"R4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"assigned_zoning_bucket\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of AIN to Effective Year from final_gdf\n",
    "ain_to_effective_year = final_gdf.set_index(\"AIN\")[\"Effective Year\"].to_dict()\n",
    "\n",
    "# Map this to the result DataFrame based on the AIN column\n",
    "result[\"Effective Year\"] = result[\"AIN\"].map(ain_to_effective_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_count = result[\"Effective Year\"].isna().sum()\n",
    "print(f\"Number of rows without a valid Effective Year: {invalid_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Zoning Tax/Acre Efficiency\n",
    "Calculate for both full dataset and post-2014 Effective Year parcels - see report for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ZONE_PREFIX and calculate aggregates\n",
    "zone_summary = {}\n",
    "\n",
    "# Get unique zone prefixes\n",
    "zone_prefixes = result[\"assigned_zoning_bucket\"].unique()\n",
    "print(zone_prefixes)\n",
    "for zone in zone_prefixes:\n",
    "    # Filter for just this zone prefix\n",
    "    zone_data = result[result[\"assigned_zoning_bucket\"] == zone]\n",
    "\n",
    "    # Skip if zone is None/NaN\n",
    "    if pd.isna(zone):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Calculate metrics with error handling\n",
    "        total_acreage = zone_data[\"acreage\"].sum()\n",
    "        total_property_tax = zone_data[\"Property_Tax_Value\"].sum()\n",
    "\n",
    "        # Filter for recent Effective Year (after 2015)\n",
    "        recent_data = zone_data[zone_data[\"Effective Year\"] > 2015]\n",
    "        total_acreage_recent = recent_data[\"acreage\"].sum()\n",
    "        total_property_tax_recent = recent_data[\"Property_Tax_Value\"].sum()\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key {e} in DataFrame. Skipping zone: {zone}\")\n",
    "        continue  # Skip this zone if keys are missing\n",
    "\n",
    "    # Avoid division by zero\n",
    "    tax_per_acre = total_property_tax / total_acreage if total_acreage > 0 else 0\n",
    "    tax_per_acre_recent = (\n",
    "        total_property_tax_recent / total_acreage_recent\n",
    "        if total_acreage_recent > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Store in dictionary\n",
    "    zone_summary[zone] = {\n",
    "        \"total_acreage\": round(total_acreage, 2),\n",
    "        \"total_property_tax\": round(total_property_tax, 2),\n",
    "        \"tax_per_acre\": round(tax_per_acre, 2),\n",
    "        \"tax_per_acre_recent\": round(tax_per_acre_recent, 2),\n",
    "    }\n",
    "\n",
    "# Now zone_summary is a dictionary with all the metrics by zone prefix\n",
    "print(zone_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tax_per_acre values, filtering out any missing values\n",
    "tax_per_acre_values = {\n",
    "    k: v[\"tax_per_acre\"] for k, v in zone_summary.items() if \"tax_per_acre\" in v\n",
    "}\n",
    "\n",
    "# Sort by tax_per_acre in descending order\n",
    "sorted_data = sorted(tax_per_acre_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display results\n",
    "for letter, tax in sorted_data:\n",
    "    print(f\"{letter}: {tax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tax_per_acre values, filtering out any missing values\n",
    "tax_per_acre_values = {\n",
    "    k: v[\"tax_per_acre_recent\"]\n",
    "    for k, v in zone_summary.items()\n",
    "    if \"tax_per_acre_recent\" in v\n",
    "}\n",
    "\n",
    "# Sort by tax_per_acre in descending order\n",
    "sorted_data = sorted(tax_per_acre_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display results\n",
    "for letter, tax in sorted_data:\n",
    "    print(f\"{letter}: {tax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells allow for exporting the merged parcel dataset and saving it back so that the full process doesn't need to be run multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_file(\"parcels_final.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_read = gpd.read_file(\"parcels_final.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = result_read\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ZIP code safely\n",
    "def extract_zip(location):\n",
    "    if not isinstance(location, str):\n",
    "        return None\n",
    "    zips = re.findall(r\"\\b\\d{5}\\b\", location)\n",
    "    return zips[-1] if zips else None\n",
    "\n",
    "\n",
    "# Create new column with ZIP code\n",
    "result[\"zip_code\"] = result[\"Property Location\"].apply(extract_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[\"zip_code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis of Parcel Up-Zoning\n",
    "Training pipeline which builds several types of regression models on the data set using different combinations of features. Goal is to predict property tax values of an upzoned property based on size and location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_outliers(df_, lower_q=0.01, upper_q=0.99):\n",
    "    \"\"\"\n",
    "    Remove outliers from the dataframe based on quantile ranges\n",
    "    \"\"\"\n",
    "    if len(df_) < 4:  # Not enough data to trim meaningfully\n",
    "        return df_\n",
    "\n",
    "    return df_[\n",
    "        df_[\"Property_Tax_Value\"].between(\n",
    "            df_[\"Property_Tax_Value\"].quantile(lower_q),\n",
    "            df_[\"Property_Tax_Value\"].quantile(upper_q),\n",
    "        )\n",
    "        & df_[\"acreage\"].between(\n",
    "            df_[\"acreage\"].quantile(lower_q),\n",
    "            df_[\"acreage\"].quantile(upper_q),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    df, min_year=0, trim_outliers_flag=False, lower_q=0.01, upper_q=0.99, use_age=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for regression modeling\n",
    "    Optionally trim outliers and choose between filtering by year or using parcel age\n",
    "    \"\"\"\n",
    "    if use_age:\n",
    "        # Use full dataset with parcel age as a feature\n",
    "        df_processed = df.copy()\n",
    "    else:\n",
    "        # Filter by year\n",
    "        df_processed = df[df[\"Effective Year\"] > min_year].copy()\n",
    "\n",
    "    print(len(df_processed))\n",
    "\n",
    "    # Drop rows with missing required fields\n",
    "    df_processed = df_processed.dropna(\n",
    "        subset=[\n",
    "            \"acreage\",\n",
    "            \"Location Latitude\",\n",
    "            \"Location Longitude\",\n",
    "            \"Property_Tax_Value\",\n",
    "            \"assigned_zoning_bucket\",\n",
    "            \"Effective Year\",  # Added to ensure Parcel_age is valid\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"After dropping NaN in required fields: {len(df_processed)}\")\n",
    "\n",
    "    print(len(df_processed))\n",
    "\n",
    "    # Always calculate parcel age for all records\n",
    "    df_processed[\"Parcel_age\"] = 2025 - df_processed[\"Effective Year\"]\n",
    "\n",
    "    # Simple fix: Drop any rows with NaN values in any column that will be used as a feature\n",
    "    # df_processed = df_processed.dropna()\n",
    "    df_processed = df_processed.dropna(\n",
    "        subset=[\n",
    "            \"acreage\",\n",
    "            \"Location Latitude\",\n",
    "            \"Location Longitude\",\n",
    "            \"Property_Tax_Value\",\n",
    "            \"assigned_zoning_bucket\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(len(df_processed))\n",
    "\n",
    "    # Trim outliers if requested\n",
    "    if trim_outliers_flag:\n",
    "        original_len = len(df_processed)\n",
    "        df_processed = trim_outliers(df_processed, lower_q, upper_q)\n",
    "        trimmed_count = original_len - len(df_processed)\n",
    "        print(\n",
    "            f\"Trimmed {trimmed_count} outliers ({trimmed_count / original_len:.1%} of data)\"\n",
    "        )\n",
    "\n",
    "    print(len(df_processed))\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def compare_regression_models(\n",
    "    df,\n",
    "    zone_prefixes,\n",
    "    feature_set=\"acreage_only\",\n",
    "    min_year=2015,\n",
    "    trim_outliers_flag=False,\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare different regression models using different feature sets\n",
    "\n",
    "    feature_set options:\n",
    "    - \"acreage_only\": just acreage (filtered by year)\n",
    "    - \"acreage_latlon\": acreage and lat/lon (filtered by year)\n",
    "    - \"acreage_zipcode\": acreage and zip code (filtered by year)\n",
    "    - \"acreage_age_latlon\": full dataset with acreage, parcel age, and lat/lon\n",
    "    - \"acreage_age_zipcode\": full dataset with acreage, parcel age, and zip code\n",
    "    \"\"\"\n",
    "    # Determine if we're using parcel age (full dataset) or filtering by year\n",
    "    use_age = \"_age\" in feature_set\n",
    "    print(\"use age\", use_age)\n",
    "    # Prepare data with appropriate features\n",
    "    df_processed = prepare_data(\n",
    "        df,\n",
    "        min_year=min_year if not use_age else 0,\n",
    "        trim_outliers_flag=trim_outliers_flag,\n",
    "        lower_q=lower_q,\n",
    "        upper_q=upper_q,\n",
    "        use_age=use_age,\n",
    "    )\n",
    "\n",
    "    # If using zip codes, add the one-hot encoding\n",
    "    if \"zipcode\" in feature_set:\n",
    "        zip_dummies = pd.get_dummies(df_processed[\"zip_code\"], prefix=\"zip\")\n",
    "        df_processed = pd.concat([df_processed, zip_dummies], axis=1)\n",
    "\n",
    "    # Models to evaluate\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "        \"Lasso Regression\": Lasso(alpha=0.1),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "        \"SVR\": Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"svr\", LinearSVR(max_iter=10000))]\n",
    "        ),\n",
    "        \"Random Forest\": RandomForestRegressor(\n",
    "            n_estimators=50, random_state=42, n_jobs=4\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(\n",
    "            n_estimators=50, random_state=42\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Skip NaN zone if present\n",
    "    valid_zones = [zone for zone in zone_prefixes if not pd.isna(zone)]\n",
    "\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Set up feature columns based on feature_set\n",
    "    base_features = [\"acreage\"]\n",
    "\n",
    "    if \"age\" in feature_set:\n",
    "        base_features.append(\"Parcel_age\")\n",
    "\n",
    "    if \"latlon\" in feature_set:\n",
    "        base_features.extend([\"Location Latitude\", \"Location Longitude\"])\n",
    "\n",
    "    # For each zone\n",
    "    for zone in valid_zones:\n",
    "        zone_data = df_processed[df_processed[\"assigned_zoning_bucket\"] == zone].copy()\n",
    "\n",
    "        # Skip if insufficient data (need at least 10 samples for meaningful results)\n",
    "        if len(zone_data) < 10:\n",
    "            print(\n",
    "                f\"Skipping zone {zone} - insufficient data (only {len(zone_data)} samples)\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing zone: {zone} (Data: {len(zone_data)} samples)\")\n",
    "\n",
    "        # Prepare features based on feature_set\n",
    "        feature_columns = base_features.copy()\n",
    "\n",
    "        # Add zip code features if needed\n",
    "        if \"zipcode\" in feature_set:\n",
    "            zip_columns = [col for col in zone_data.columns if col.startswith(\"zip_\")]\n",
    "            feature_columns.extend(zip_columns)\n",
    "\n",
    "        # Check for NaN values in feature columns\n",
    "        print(\"NaN values in feature columns before filtering:\")\n",
    "        for col in feature_columns:\n",
    "            nan_count = zone_data[col].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"  {col}: {nan_count} NaNs\")\n",
    "\n",
    "        # Filter rows with any NaN in feature columns\n",
    "        zone_data = zone_data.dropna(subset=feature_columns)\n",
    "        print(f\"After removing NaNs in features: {len(zone_data)} samples\")\n",
    "\n",
    "        # Skip if insufficient data after NaN removal\n",
    "        if len(zone_data) < 10:\n",
    "            print(f\"Skipping zone {zone} after NaN removal - insufficient data\")\n",
    "            continue\n",
    "\n",
    "        X = zone_data[feature_columns].values\n",
    "        y = zone_data[\"Property_Tax_Value\"].values\n",
    "\n",
    "        # Try to split data for validation - 70% train, 30% test\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "            zone_results = []\n",
    "\n",
    "            # Evaluate each model\n",
    "            for name, model in models.items():\n",
    "                try:\n",
    "                    # Train model\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    # Make predictions\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    r2 = r2_score(y_test, y_pred)\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                    # Cross validation for robustness (ensure we have enough folds)\n",
    "                    cv_folds = min(\n",
    "                        5, len(zone_data) // 3\n",
    "                    )  # Ensure at least 3 samples per fold\n",
    "                    if cv_folds >= 2:  # Need at least 2 folds\n",
    "                        cv_scores = cross_val_score(\n",
    "                            model, X, y, cv=cv_folds, scoring=\"r2\"\n",
    "                        )\n",
    "                        cv_r2 = cv_scores.mean()\n",
    "                    else:\n",
    "                        cv_r2 = np.nan  # Cannot perform CV with too few samples\n",
    "\n",
    "                    zone_results.append(\n",
    "                        {\n",
    "                            \"zone\": zone,\n",
    "                            \"model\": name,\n",
    "                            \"r2_score\": r2,\n",
    "                            \"cv_r2_score\": cv_r2,\n",
    "                            \"rmse\": rmse,\n",
    "                            \"mae\": mae,\n",
    "                            \"samples\": len(zone_data),\n",
    "                            \"feature_set\": feature_set,\n",
    "                            \"num_features\": X.shape[1],\n",
    "                            \"outliers_trimmed\": trim_outliers_flag,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        f\"  {name}: R² = {r2:.4f}, CV R² = {cv_r2:.4f}, RMSE = {rmse:.2f}, MAE = {mae:.2f}\"\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with {name}: {str(e)}\")\n",
    "\n",
    "            results[zone] = zone_results\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"  Error splitting data for zone {zone}: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_results(results, feature_set):\n",
    "    \"\"\"\n",
    "    Summarize regression results and create visualizations\n",
    "    \"\"\"\n",
    "    # Flatten results into a dataframe\n",
    "    all_results = []\n",
    "    for zone, models in results.items():\n",
    "        all_results.extend(models)\n",
    "\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    if df_results.empty:\n",
    "        print(\"No results to summarize.\")\n",
    "        return df_results\n",
    "\n",
    "    # Print best model for each zone\n",
    "    print(f\"\\n--- BEST MODEL FOR EACH ZONING BUCKET (Feature Set: {feature_set}) ---\")\n",
    "    for zone in df_results[\"zone\"].unique():\n",
    "        zone_df = df_results[df_results[\"zone\"] == zone]\n",
    "\n",
    "        # Find index of best model based on R²\n",
    "        if not zone_df.empty and not all(pd.isna(zone_df[\"r2_score\"])):\n",
    "            best_idx = zone_df[\"r2_score\"].idxmax()\n",
    "            if pd.notna(best_idx):  # Ensure we have a valid index\n",
    "                best_model = zone_df.loc[best_idx]\n",
    "                outlier_status = (\n",
    "                    \"with outliers trimmed\"\n",
    "                    if best_model.get(\"outliers_trimmed\", False)\n",
    "                    else \"all data\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"\\nZONE: {zone} (Samples: {best_model['samples']}, Features: {best_model['num_features']}, {outlier_status})\"\n",
    "                )\n",
    "                print(f\"Best model: {best_model['model']}\")\n",
    "                print(f\"R² Score: {best_model['r2_score']:.4f}\")\n",
    "                if pd.notna(best_model[\"cv_r2_score\"]):\n",
    "                    print(f\"CV R² Score: {best_model['cv_r2_score']:.4f}\")\n",
    "                else:\n",
    "                    print(\"CV R² Score: N/A (insufficient data for cross-validation)\")\n",
    "                print(f\"RMSE: {best_model['rmse']:.2f}\")\n",
    "                print(f\"MAE: {best_model['mae']:.2f}\")\n",
    "\n",
    "    # Plot R² comparison across zones and models\n",
    "    for zone in df_results[\"zone\"].unique():\n",
    "        zone_df = df_results[df_results[\"zone\"] == zone]\n",
    "\n",
    "        if zone_df.empty:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Sort by R² score\n",
    "        zone_df = zone_df.sort_values(\"r2_score\", ascending=False)\n",
    "\n",
    "        plt.bar(zone_df[\"model\"], zone_df[\"r2_score\"], alpha=0.7)\n",
    "        plt.axhline(\n",
    "            y=0.7, color=\"r\", linestyle=\"--\", label=\"R² = 0.7 (Good fit threshold)\"\n",
    "        )\n",
    "\n",
    "        # Title with feature set and outlier info\n",
    "        outlier_suffix = (\n",
    "            \" (Outliers Trimmed)\"\n",
    "            if zone_df.iloc[0].get(\"outliers_trimmed\", False)\n",
    "            else \"\"\n",
    "        )\n",
    "        plt.title(\n",
    "            f\"Model R² Comparison for Zone: {zone} - {feature_set}{outlier_suffix}\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\"Model\")\n",
    "        plt.ylabel(\"R² Score\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save with descriptive filename\n",
    "        outlier_filename = (\n",
    "            \"_trimmed\" if zone_df.iloc[0].get(\"outliers_trimmed\", False) else \"\"\n",
    "        )\n",
    "        # plt.savefig(\n",
    "        #     f\"model_comparison_zone_{zone}_{feature_set}{outlier_filename}.png\",\n",
    "        #     dpi=300,\n",
    "        #     bbox_inches=\"tight\",\n",
    "        # )\n",
    "        plt.close()\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def find_best_models_multi_metric(\n",
    "    best_df, weight_r2=0.3, weight_cv_r2=0.4, weight_rmse=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the best model for each zone using a weighted combination of R², CV R², and RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_df : pandas DataFrame\n",
    "        DataFrame containing model results with columns: 'zone', 'feature_set', 'best_model',\n",
    "        'r2_score', 'cv_r2_score', 'rmse', and 'samples'\n",
    "    weight_r2 : float, default=0.3\n",
    "        Weight for R² score (higher is better)\n",
    "    weight_cv_r2 : float, default=0.4\n",
    "        Weight for CV R² score (higher is better)\n",
    "    weight_rmse : float, default=0.3\n",
    "        Weight for negative RMSE (lower RMSE is better)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary with zone keys and best model information as values\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    total_weight = weight_r2 + weight_cv_r2 + weight_rmse\n",
    "    if not np.isclose(total_weight, 1.0):\n",
    "        print(f\"Warning: Weights sum to {total_weight}, not 1.0. Normalizing weights.\")\n",
    "        weight_r2 /= total_weight\n",
    "        weight_cv_r2 /= total_weight\n",
    "        weight_rmse /= total_weight\n",
    "\n",
    "    print(\"\\n--- OVERALL BEST MODEL & FEATURE SET FOR EACH ZONE (MULTI-METRIC) ---\")\n",
    "    print(\n",
    "        f\"Weights: R² = {weight_r2:.2f}, CV R² = {weight_cv_r2:.2f}, RMSE = {weight_rmse:.2f}\"\n",
    "    )\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    for zone in best_df[\"zone\"].unique():\n",
    "        zone_best_df = best_df[best_df[\"zone\"] == zone].copy()\n",
    "\n",
    "        if zone_best_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Handle NaN values in metrics\n",
    "        zone_best_df[\"cv_r2_score\"] = zone_best_df[\"cv_r2_score\"].fillna(\n",
    "            zone_best_df[\"r2_score\"] * 0.8\n",
    "        )\n",
    "\n",
    "        # Normalize metrics to [0,1] range for fair comparison\n",
    "        # For R² and CV R², higher is better (1 is perfect)\n",
    "        # For RMSE, lower is better (0 is perfect)\n",
    "\n",
    "        if len(zone_best_df) > 1:  # Only normalize if more than one model\n",
    "            # For R² scores: normalize to [0,1] where 1 is best (highest R²)\n",
    "            r2_min = zone_best_df[\"r2_score\"].min()\n",
    "            r2_max = zone_best_df[\"r2_score\"].max()\n",
    "            r2_range = r2_max - r2_min\n",
    "\n",
    "            cv_r2_min = zone_best_df[\"cv_r2_score\"].min()\n",
    "            cv_r2_max = zone_best_df[\"cv_r2_score\"].max()\n",
    "            cv_r2_range = cv_r2_max - cv_r2_min\n",
    "\n",
    "            # For RMSE: normalize to [0,1] where 1 is best (lowest RMSE)\n",
    "            rmse_min = zone_best_df[\"rmse\"].min()\n",
    "            rmse_max = zone_best_df[\"rmse\"].max()\n",
    "            rmse_range = rmse_max - rmse_min\n",
    "\n",
    "            # Avoid division by zero\n",
    "            if r2_range > 0:\n",
    "                zone_best_df[\"r2_norm\"] = (zone_best_df[\"r2_score\"] - r2_min) / r2_range\n",
    "            else:\n",
    "                zone_best_df[\"r2_norm\"] = 1.0  # All values are the same\n",
    "\n",
    "            if cv_r2_range > 0:\n",
    "                zone_best_df[\"cv_r2_norm\"] = (\n",
    "                    zone_best_df[\"cv_r2_score\"] - cv_r2_min\n",
    "                ) / cv_r2_range\n",
    "            else:\n",
    "                zone_best_df[\"cv_r2_norm\"] = 1.0  # All values are the same\n",
    "\n",
    "            if rmse_range > 0:\n",
    "                # Invert RMSE so that lower is better (1 - normalized value)\n",
    "                zone_best_df[\"rmse_norm\"] = 1 - (\n",
    "                    (zone_best_df[\"rmse\"] - rmse_min) / rmse_range\n",
    "                )\n",
    "            else:\n",
    "                zone_best_df[\"rmse_norm\"] = 1.0  # All values are the same\n",
    "        else:\n",
    "            # If only one model, set normalized values to 1\n",
    "            zone_best_df[\"r2_norm\"] = 1.0\n",
    "            zone_best_df[\"cv_r2_norm\"] = 1.0\n",
    "            zone_best_df[\"rmse_norm\"] = 1.0\n",
    "\n",
    "        # Calculate weighted composite score\n",
    "        zone_best_df[\"composite_score\"] = (\n",
    "            weight_r2 * zone_best_df[\"r2_norm\"]\n",
    "            + weight_cv_r2 * zone_best_df[\"cv_r2_norm\"]\n",
    "            + weight_rmse * zone_best_df[\"rmse_norm\"]\n",
    "        )\n",
    "\n",
    "        # Find the model with the highest composite score\n",
    "        overall_best_idx = zone_best_df[\"composite_score\"].idxmax()\n",
    "        overall_best = zone_best_df.loc[overall_best_idx]\n",
    "\n",
    "        # Print detailed results\n",
    "        print(f\"\\nZONE: {zone} (Samples: {overall_best['samples']})\")\n",
    "        print(f\"Best feature set: {overall_best['feature_set']}\")\n",
    "        print(f\"Best model: {overall_best['best_model']}\")\n",
    "        print(\n",
    "            f\"R² Score: {overall_best['r2_score']:.4f} (normalized: {overall_best['r2_norm']:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"CV R² Score: {overall_best['cv_r2_score']:.4f} (normalized: {overall_best['cv_r2_norm']:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"RMSE: {overall_best['rmse']:.2f} (normalized: {overall_best['rmse_norm']:.4f})\"\n",
    "        )\n",
    "        print(f\"Composite Score: {overall_best['composite_score']:.4f}\")\n",
    "\n",
    "        # Add to results dictionary\n",
    "        best_models[zone] = overall_best.to_dict()\n",
    "\n",
    "        # Top 3 models by composite score for this zone\n",
    "        print(\"\\nTop models by composite score:\")\n",
    "        top_models = zone_best_df.sort_values(\"composite_score\", ascending=False).head(\n",
    "            3\n",
    "        )\n",
    "\n",
    "        for i, (_, model) in enumerate(top_models.iterrows(), 1):\n",
    "            print(\n",
    "                f\"{i}. {model['best_model']} ({model['feature_set']}): \"\n",
    "                f\"Score = {model['composite_score']:.4f}, \"\n",
    "                f\"R² = {model['r2_score']:.4f}, \"\n",
    "                f\"CV R² = {model['cv_r2_score']:.4f}, \"\n",
    "                f\"RMSE = {model['rmse']:.2f}\"\n",
    "            )\n",
    "\n",
    "    return best_models\n",
    "\n",
    "\n",
    "def visualize_metric_comparison(best_df, zone, metrics=None):\n",
    "    \"\"\"\n",
    "    Create a parallel coordinates plot to visualize multiple metrics across models\n",
    "    for a specific zone.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_df : pandas DataFrame\n",
    "        DataFrame containing model results\n",
    "    zone : str\n",
    "        The zone to visualize\n",
    "    metrics : list, optional\n",
    "        List of metrics to include. Defaults to ['r2_score', 'cv_r2_score', 'rmse']\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas.plotting import parallel_coordinates\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = [\"r2_score\", \"cv_r2_score\", \"rmse\"]\n",
    "\n",
    "    zone_df = best_df[best_df[\"zone\"] == zone].copy()\n",
    "\n",
    "    if len(zone_df) < 2:\n",
    "        print(f\"Not enough models for zone {zone} to create comparison visualization\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for parallel coordinates plot\n",
    "    plot_df = zone_df[[\"feature_set\", \"best_model\"] + metrics].copy()\n",
    "\n",
    "    # Create a class column combining feature set and model\n",
    "    plot_df[\"Model\"] = plot_df[\"feature_set\"] + \" + \" + plot_df[\"best_model\"]\n",
    "\n",
    "    # For RMSE, lower is better, so we'll negate it for visualization\n",
    "    if \"rmse\" in metrics:\n",
    "        max_rmse = plot_df[\"rmse\"].max()\n",
    "        plot_df[\"neg_rmse\"] = -1 * plot_df[\"rmse\"] + max_rmse\n",
    "        metrics = [m if m != \"rmse\" else \"neg_rmse\" for m in metrics]\n",
    "\n",
    "    # Create the parallel coordinates plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    parallel_coordinates(plot_df, \"Model\", cols=metrics, colormap=\"viridis\")\n",
    "\n",
    "    title = f\"Model Comparison for Zone {zone} Across Multiple Metrics\"\n",
    "    if \"neg_rmse\" in metrics:\n",
    "        plt.text(\n",
    "            0.5,\n",
    "            -0.1,\n",
    "            \"Note: RMSE is inverted so higher is better for all metrics\",\n",
    "            ha=\"center\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=10,\n",
    "            fontstyle=\"italic\",\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Return the plot for display\n",
    "    return plt\n",
    "\n",
    "\n",
    "def run_analysis_combinations_rmse(df, zone_prefixes, trim_outliers_flag=False):\n",
    "    \"\"\"\n",
    "    Run analysis for all the specified feature combinations\n",
    "    \"\"\"\n",
    "    # Define the feature combinations to test\n",
    "    feature_combinations = [\n",
    "        {\n",
    "            \"name\": \"acreage_only\",\n",
    "            \"description\": \"Filter 2015, use acreage\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_latlon\",\n",
    "            \"description\": \"Filter 2015, use acreage, latitude/longitude\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_zipcode\",\n",
    "            \"description\": \"Filter 2015, use acreage, zip code encoded\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_age_latlon\",\n",
    "            \"description\": \"Full data set, acreage, use parcel age, latitude/longitude\",\n",
    "            \"min_year\": 0,  # Use all data\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_age_zipcode\",\n",
    "            \"description\": \"Full data set, acreage, parcel age, zip code encoded\",\n",
    "            \"min_year\": 0,  # Use all data\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Run analysis for each feature combination\n",
    "    for combo in feature_combinations:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"ANALYZING: {combo['description']}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        # Run the analysis\n",
    "        results = compare_regression_models(\n",
    "            df,\n",
    "            zone_prefixes,\n",
    "            feature_set=combo[\"name\"],\n",
    "            min_year=combo[\"min_year\"],\n",
    "            trim_outliers_flag=trim_outliers_flag,\n",
    "        )\n",
    "\n",
    "        # Summarize the results\n",
    "        summary_df = summarize_results(results, combo[\"name\"])\n",
    "\n",
    "        # Add to overall results\n",
    "        all_results.append(summary_df)\n",
    "\n",
    "        # Save individual results\n",
    "        outlier_suffix = \"_trimmed\" if trim_outliers_flag else \"\"\n",
    "        # summary_df.to_csv(\n",
    "        #     f\"regression_results_{combo['name']}{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "    # Combine all results for comparison\n",
    "    if all_results:\n",
    "        combined_df = pd.concat(all_results)\n",
    "        # combined_df.to_csv(\n",
    "        #     f\"combined_regression_results{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "        # Create a summary of best models for each zone and feature set\n",
    "        best_models = []\n",
    "        for zone in combined_df[\"zone\"].unique():\n",
    "            for feature_set in combined_df[\"feature_set\"].unique():\n",
    "                zone_feature_df = combined_df[\n",
    "                    (combined_df[\"zone\"] == zone)\n",
    "                    & (combined_df[\"feature_set\"] == feature_set)\n",
    "                ]\n",
    "\n",
    "                if not zone_feature_df.empty and not all(\n",
    "                    pd.isna(zone_feature_df[\"r2_score\"])\n",
    "                ):\n",
    "                    best_idx = zone_feature_df[\"r2_score\"].idxmax()\n",
    "                    if pd.notna(best_idx):\n",
    "                        best_model = zone_feature_df.loc[best_idx]\n",
    "                        best_models.append(\n",
    "                            {\n",
    "                                \"zone\": zone,\n",
    "                                \"feature_set\": feature_set,\n",
    "                                \"best_model\": best_model[\"model\"],\n",
    "                                \"r2_score\": best_model[\"r2_score\"],\n",
    "                                \"cv_r2_score\": best_model[\"cv_r2_score\"],\n",
    "                                \"rmse\": best_model[\"rmse\"],\n",
    "                                \"samples\": best_model[\"samples\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        best_df = pd.DataFrame(best_models)\n",
    "        print(best_df)\n",
    "        # best_df.to_csv(\n",
    "        #     f\"best_models_by_zone_and_features{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "        # Use the multi-metric approach for selecting the best models\n",
    "        print(\"\\n=== SINGLE METRIC MODEL SELECTION ===\")\n",
    "        print(\"\\n1. Using only R² Score:\")\n",
    "        for zone in best_df[\"zone\"].unique():\n",
    "            zone_best_df = best_df[best_df[\"zone\"] == zone]\n",
    "            if not zone_best_df.empty:\n",
    "                overall_best_idx = zone_best_df[\"r2_score\"].idxmax()\n",
    "                overall_best = zone_best_df.loc[overall_best_idx]\n",
    "                print(f\"\\nZONE: {zone}\")\n",
    "                print(f\"Best feature set: {overall_best['feature_set']}\")\n",
    "                print(f\"Best model: {overall_best['best_model']}\")\n",
    "                print(f\"R² Score: {overall_best['r2_score']:.4f}\")\n",
    "                print(f\"CV R² Score: {overall_best['cv_r2_score']:.4f}\")\n",
    "                print(f\"RMSE: {overall_best['rmse']:.2f}\")\n",
    "\n",
    "        print(\"\\n2. Using only RMSE:\")\n",
    "        for zone in best_df[\"zone\"].unique():\n",
    "            zone_best_df = best_df[best_df[\"zone\"] == zone]\n",
    "            if not zone_best_df.empty:\n",
    "                overall_best_idx = zone_best_df[\"rmse\"].idxmin()\n",
    "                overall_best = zone_best_df.loc[overall_best_idx]\n",
    "                print(f\"\\nZONE: {zone}\")\n",
    "                print(f\"Best feature set: {overall_best['feature_set']}\")\n",
    "                print(f\"Best model: {overall_best['best_model']}\")\n",
    "                print(f\"R² Score: {overall_best['r2_score']:.4f}\")\n",
    "                print(f\"CV R² Score: {overall_best['cv_r2_score']:.4f}\")\n",
    "                print(f\"RMSE: {overall_best['rmse']:.2f}\")\n",
    "\n",
    "        print(\"\\n=== MULTI-METRIC MODEL SELECTION ===\")\n",
    "        # Call the multi-metric selection function with different weight configurations\n",
    "        print(\"\\nBalanced weighting (33% each):\")\n",
    "        best_models_balanced = find_best_models_multi_metric(\n",
    "            best_df, weight_r2=0.33, weight_cv_r2=0.34, weight_rmse=0.33\n",
    "        )\n",
    "\n",
    "        print(\"\\nCross-validation emphasis (50% CV R²):\")\n",
    "        best_models_cv = find_best_models_multi_metric(\n",
    "            best_df, weight_r2=0.25, weight_cv_r2=0.50, weight_rmse=0.25\n",
    "        )\n",
    "\n",
    "        print(\"\\nRMSE emphasis (50% RMSE):\")\n",
    "        best_models_rmse = find_best_models_multi_metric(\n",
    "            best_df, weight_r2=0.25, weight_cv_r2=0.25, weight_rmse=0.50\n",
    "        )\n",
    "\n",
    "        # Create comparison plot of best R² scores by feature set for each zone\n",
    "        for zone in best_df[\"zone\"].unique():\n",
    "            zone_best_df = best_df[best_df[\"zone\"] == zone]\n",
    "            if (\n",
    "                len(zone_best_df) >= 2\n",
    "            ):  # Only create plot if we have at least 2 feature sets\n",
    "                plt.figure(figsize=(12, 6))\n",
    "\n",
    "                # Sort by R² score\n",
    "                zone_best_df = zone_best_df.sort_values(\"r2_score\", ascending=False)\n",
    "\n",
    "                # Create bar chart\n",
    "                bars = plt.bar(\n",
    "                    zone_best_df[\"feature_set\"], zone_best_df[\"r2_score\"], alpha=0.7\n",
    "                )\n",
    "\n",
    "                # Add model name to each bar\n",
    "                for i, bar in enumerate(bars):\n",
    "                    model_name = zone_best_df.iloc[i][\"best_model\"]\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width() / 2,\n",
    "                        0.02,\n",
    "                        model_name,\n",
    "                        ha=\"center\",\n",
    "                        rotation=90,\n",
    "                        color=\"white\",\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "                plt.axhline(\n",
    "                    y=0.7,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"--\",\n",
    "                    label=\"R² = 0.7 (Good fit threshold)\",\n",
    "                )\n",
    "                plt.title(\n",
    "                    f\"Feature Set Comparison for Zone: {zone}{outlier_suffix}\",\n",
    "                    fontsize=14,\n",
    "                )\n",
    "                plt.xlabel(\"Feature Set\")\n",
    "                plt.ylabel(\"Best R² Score\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                plt.ylim(0, 1.0)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # plt.savefig(\n",
    "                #     f\"feature_set_comparison_zone_{zone}{outlier_suffix}.png\",\n",
    "                #     dpi=300,\n",
    "                #     bbox_inches=\"tight\",\n",
    "                # )\n",
    "                plt.close()\n",
    "\n",
    "    return combined_df if all_results else None\n",
    "\n",
    "\n",
    "def run_analysis_combinations(df, zone_prefixes, trim_outliers_flag=False):\n",
    "    \"\"\"\n",
    "    Run analysis for all the specified feature combinations\n",
    "    \"\"\"\n",
    "    # Define the feature combinations to test\n",
    "    feature_combinations = [\n",
    "        {\n",
    "            \"name\": \"acreage_only\",\n",
    "            \"description\": \"Filter 2015, use acreage\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_latlon\",\n",
    "            \"description\": \"Filter 2015, use acreage, latitude/longitude\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_zipcode\",\n",
    "            \"description\": \"Filter 2015, use acreage, zip code encoded\",\n",
    "            \"min_year\": 2015,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_age_latlon\",\n",
    "            \"description\": \"Full data set, acreage, use parcel age, latitude/longitude\",\n",
    "            \"min_year\": 0,  # Use all data\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"acreage_age_zipcode\",\n",
    "            \"description\": \"Full data set, acreage, parcel age, zip code encoded\",\n",
    "            \"min_year\": 0,  # Use all data\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Run analysis for each feature combination\n",
    "    for combo in feature_combinations:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"ANALYZING: {combo['description']}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        # Run the analysis\n",
    "        results = compare_regression_models(\n",
    "            df,\n",
    "            zone_prefixes,\n",
    "            feature_set=combo[\"name\"],\n",
    "            min_year=combo[\"min_year\"],\n",
    "            trim_outliers_flag=trim_outliers_flag,\n",
    "        )\n",
    "\n",
    "        # Summarize the results\n",
    "        summary_df = summarize_results(results, combo[\"name\"])\n",
    "\n",
    "        # Add to overall results\n",
    "        all_results.append(summary_df)\n",
    "\n",
    "        # Save individual results\n",
    "        outlier_suffix = \"_trimmed\" if trim_outliers_flag else \"\"\n",
    "        # summary_df.to_csv(\n",
    "        #     f\"regression_results_{combo['name']}{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "    # Combine all results for comparison\n",
    "    if all_results:\n",
    "        combined_df = pd.concat(all_results)\n",
    "        # combined_df.to_csv(\n",
    "        #     f\"combined_regression_results{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "        # Create a summary of best models for each zone and feature set\n",
    "        best_models = []\n",
    "        for zone in combined_df[\"zone\"].unique():\n",
    "            for feature_set in combined_df[\"feature_set\"].unique():\n",
    "                zone_feature_df = combined_df[\n",
    "                    (combined_df[\"zone\"] == zone)\n",
    "                    & (combined_df[\"feature_set\"] == feature_set)\n",
    "                ]\n",
    "\n",
    "                if not zone_feature_df.empty and not all(\n",
    "                    pd.isna(zone_feature_df[\"r2_score\"])\n",
    "                ):\n",
    "                    best_idx = zone_feature_df[\"r2_score\"].idxmax()\n",
    "                    if pd.notna(best_idx):\n",
    "                        best_model = zone_feature_df.loc[best_idx]\n",
    "                        best_models.append(\n",
    "                            {\n",
    "                                \"zone\": zone,\n",
    "                                \"feature_set\": feature_set,\n",
    "                                \"best_model\": best_model[\"model\"],\n",
    "                                \"r2_score\": best_model[\"r2_score\"],\n",
    "                                \"cv_r2_score\": best_model[\"cv_r2_score\"],\n",
    "                                \"rmse\": best_model[\"rmse\"],\n",
    "                                \"samples\": best_model[\"samples\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        best_df = pd.DataFrame(best_models)\n",
    "        print(best_df)\n",
    "        # best_df.to_csv(\n",
    "        #     f\"best_models_by_zone_and_features{outlier_suffix}.csv\", index=False\n",
    "        # )\n",
    "\n",
    "        # For each zone, find the overall best feature set/model combination\n",
    "        print(\"\\n--- OVERALL BEST MODEL & FEATURE SET FOR EACH ZONE ---\")\n",
    "        for zone in best_df[\"zone\"].unique():\n",
    "            zone_best_df = best_df[best_df[\"zone\"] == zone]\n",
    "            if not zone_best_df.empty:\n",
    "                overall_best_idx = zone_best_df[\"r2_score\"].idxmax()\n",
    "                overall_best = zone_best_df.loc[overall_best_idx]\n",
    "                print(f\"\\nZONE: {zone}\")\n",
    "                print(f\"Best feature set: {overall_best['feature_set']}\")\n",
    "                print(f\"Best model: {overall_best['best_model']}\")\n",
    "                print(f\"R² Score: {overall_best['r2_score']:.4f}\")\n",
    "                print(f\"CV R² Score: {overall_best['cv_r2_score']:.4f}\")\n",
    "                print(f\"RMSE: {overall_best['rmse']:.2f}\")\n",
    "\n",
    "        # Create comparison plot of best R² scores by feature set for each zone\n",
    "        for zone in best_df[\"zone\"].unique():\n",
    "            zone_best_df = best_df[best_df[\"zone\"] == zone]\n",
    "            if (\n",
    "                len(zone_best_df) >= 2\n",
    "            ):  # Only create plot if we have at least 2 feature sets\n",
    "                plt.figure(figsize=(12, 6))\n",
    "\n",
    "                # Sort by R² score\n",
    "                zone_best_df = zone_best_df.sort_values(\"r2_score\", ascending=False)\n",
    "\n",
    "                # Create bar chart\n",
    "                bars = plt.bar(\n",
    "                    zone_best_df[\"feature_set\"], zone_best_df[\"r2_score\"], alpha=0.7\n",
    "                )\n",
    "\n",
    "                # Add model name to each bar\n",
    "                for i, bar in enumerate(bars):\n",
    "                    model_name = zone_best_df.iloc[i][\"best_model\"]\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width() / 2,\n",
    "                        0.02,\n",
    "                        model_name,\n",
    "                        ha=\"center\",\n",
    "                        rotation=90,\n",
    "                        color=\"white\",\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "                plt.axhline(\n",
    "                    y=0.7,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"--\",\n",
    "                    label=\"R² = 0.7 (Good fit threshold)\",\n",
    "                )\n",
    "                plt.title(\n",
    "                    f\"Feature Set Comparison for Zone: {zone}{outlier_suffix}\",\n",
    "                    fontsize=14,\n",
    "                )\n",
    "                plt.xlabel(\"Feature Set\")\n",
    "                plt.ylabel(\"Best R² Score\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                plt.ylim(0, 1.0)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # plt.savefig(\n",
    "                #     f\"feature_set_comparison_zone_{zone}{outlier_suffix}.png\",\n",
    "                #     dpi=300,\n",
    "                #     bbox_inches=\"tight\",\n",
    "                # )\n",
    "                plt.close()\n",
    "\n",
    "    return combined_df if all_results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main(df, zone_prefixes):\n",
    "    # Run analysis with default settings (no outlier trimming)\n",
    "    print(\"\\nRunning analysis without outlier trimming...\")\n",
    "    combined_results = run_analysis_combinations_rmse(\n",
    "        df, zone_prefixes, trim_outliers_flag=False\n",
    "    )\n",
    "\n",
    "    # Run analysis with outlier trimming\n",
    "    print(\"\\nRunning analysis with outlier trimming...\")\n",
    "    combined_results_trimmed = run_analysis_combinations_rmse(\n",
    "        df, zone_prefixes, trim_outliers_flag=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"\\nAnalysis complete! Check the CSV files and PNG visualizations for detailed results.\"\n",
    "    )\n",
    "\n",
    "    return combined_results, combined_results_trimmed\n",
    "\n",
    "\n",
    "# To execute this script:\n",
    "combined_results, combined_results_trimmed = main(result, [\"R3\", \"R4\", \"R5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"tax_per_acre_parcel\"] = result[\"Property_Tax_Value\"] / result[\"acreage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_data = result[result[\"assigned_zoning_bucket\"] == \"R1\"]\n",
    "threshold = r1_data[\"tax_per_acre_parcel\"].quantile(0.99)\n",
    "trimmed = r1_data[r1_data[\"tax_per_acre_parcel\"] <= threshold]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(trimmed[\"tax_per_acre_parcel\"], bins=50, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Trimmed Histogram of Tax per Acre (Below 99th Percentile) for R1 Parcels\")\n",
    "plt.xlabel(\"Tax per Acre\")\n",
    "plt.ylabel(\"Number of Parcels\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tax_per_acre and tax_per_acre_recent to result\n",
    "result[\"tax_per_acre_zone\"] = result[\"assigned_zoning_bucket\"].map(\n",
    "    lambda z: zone_summary.get(z, {}).get(\"tax_per_acre\", None)\n",
    ")\n",
    "result[\"tax_per_acre_zone_recent\"] = result[\"assigned_zoning_bucket\"].map(\n",
    "    lambda z: zone_summary.get(z, {}).get(\"tax_per_acre_recent\", None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"Property_Tax_Value\"].sum() * 0.2478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"acreage\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_file(\"final_parcels_merged.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_file(\"final_parcels_merged.shp\", driver=\"ESRI Shapefile\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SB 79 Parcel Analysis\n",
    "Import SB 79 relevant transit stops geospatial data and use efficiency metrics and regression models to approximate tax increase for affected parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the station shapefile\n",
    "stations = gpd.read_file(\"data/Metro_Stops_SB_79.geojson\")\n",
    "stations = stations[[\"79_Tier\", \"geometry\"]]\n",
    "\n",
    "# Ensure the GeoDataFrame has a projected CRS in feet/meters for buffering (1 mile ≈ 1609.34 meters)\n",
    "if stations.crs is None or stations.crs.is_geographic:\n",
    "    # Reproject to a projected CRS suitable for LA (e.g., EPSG:6420 or 2229)\n",
    "    stations = stations.to_crs(\n",
    "        epsg=6420\n",
    "    )  # NAD83 / California zone 5 (ftUS) or 2229 depending on your data\n",
    "\n",
    "tier_1 = stations[stations[\"79_Tier\"] == 1]\n",
    "tier_2 = stations[stations[\"79_Tier\"] == 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create catchment buffers to determine parcels affected by SB 79 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-mile buffers around each station (1 mile ≈ 5280 feet)\n",
    "catchment_buffers_1 = tier_1.copy()\n",
    "catchment_buffers_1[\"geometry\"] = tier_1.buffer(2640)  # 1/2 mile buffer in feet\n",
    "\n",
    "catchment_buffers_2 = tier_2.copy()\n",
    "catchment_buffers_2[\"geometry\"] = tier_2.buffer(2640)  # 1/2 mile buffer in feet\n",
    "\n",
    "catchment_buffers_1_q = tier_1.copy()\n",
    "catchment_buffers_1_q[\"geometry\"] = tier_1.buffer(1320)  # 1/2 mile buffer in feet\n",
    "\n",
    "catchment_buffers_2_q = tier_2.copy()\n",
    "catchment_buffers_2_q[\"geometry\"] = tier_2.buffer(1320)  # 1/2 mile buffer in feet\n",
    "\n",
    "\n",
    "# Optional: merge all buffers into a single multipolygon for filtering\n",
    "catchment_union_1_h = catchment_buffers_1.unary_union\n",
    "catchment_union_2_h = catchment_buffers_2.unary_union\n",
    "catchment_union_1_q = catchment_buffers_1_q.unary_union\n",
    "catchment_union_2_q = catchment_buffers_2_q.unary_union\n",
    "\n",
    "\n",
    "catchment_gdf_1_h = gpd.GeoDataFrame(\n",
    "    geometry=[catchment_union_1_h], crs=catchment_buffers_1.crs\n",
    ")\n",
    "\n",
    "# Ensure other_gdf is in the same CRS\n",
    "other_gdf = result.to_crs(catchment_gdf_1_h.crs)\n",
    "# Ensure both are in the same CRS (should be, if projected correctly earlier)\n",
    "assert other_gdf.crs == catchment_gdf_1_h.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_1_qtr = catchment_union_1_q\n",
    "union_1_half = catchment_union_1_h\n",
    "union_2_qtr = catchment_union_2_q\n",
    "union_2_half = catchment_union_2_h\n",
    "\n",
    "\n",
    "def determine_upzoned_zone(geom):\n",
    "    if geom.intersects(union_1_qtr):\n",
    "        return \"R5\"\n",
    "    elif geom.intersects(union_1_half):\n",
    "        return \"R4\"\n",
    "    elif geom.intersects(union_2_qtr):\n",
    "        return \"R4\"\n",
    "    elif geom.intersects(union_2_half):\n",
    "        return \"R3\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# other_gdf[\"upzoned\"] = other_gdf.geometry.apply(determine_upzoned_zone)\n",
    "\n",
    "tqdm.pandas()  # This monkey-patches pandas to support progress_apply\n",
    "\n",
    "# Use progress_apply instead of apply\n",
    "other_gdf[\"upzoned\"] = other_gdf.geometry.progress_apply(determine_upzoned_zone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_gdf = other_gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_gdf = copy_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_gdf[\"upzoned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define zoning rank (higher = more intense)\n",
    "zoning_rank = {\"Parking\": 0, \"R1\": 1, \"R2\": 2, \"R3\": 3, \"R4\": 4, \"R5\": 5}\n",
    "\n",
    "# Assign ranks for comparison\n",
    "other_gdf[\"assigned_rank\"] = other_gdf[\"assigned_zoning_bucket\"].map(zoning_rank)\n",
    "other_gdf[\"upzoned_rank\"] = other_gdf[\"upzoned\"].map(zoning_rank)\n",
    "\n",
    "# Null out any proposed upzoning that would downzone or do nothing\n",
    "# (optional: if you want to allow same-zone upzones, use < instead of <=)\n",
    "other_gdf.loc[other_gdf[\"upzoned_rank\"] <= other_gdf[\"assigned_rank\"], \"upzoned\"] = None\n",
    "\n",
    "# Drop the temporary rank columns if you want\n",
    "other_gdf.drop(columns=[\"assigned_rank\", \"upzoned_rank\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_gdf[\"upzoned\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate tax increase using tax/acre metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Total acres by original zoning bucket\n",
    "acres_by_original_zone = (\n",
    "    other_gdf.groupby(\"assigned_zoning_bucket\")[\"acreage\"].sum().reset_index()\n",
    ")\n",
    "acres_by_original_zone.columns = [\"zoning_bucket\", \"total_acres\"]\n",
    "\n",
    "# 2. Create a fallback zoning bucket using 'upzoned' if it exists, else 'assigned_zoning_bucket'\n",
    "other_gdf[\"zoning_bucket_with_upzoning\"] = other_gdf[\"upzoned\"].fillna(\n",
    "    other_gdf[\"assigned_zoning_bucket\"]\n",
    ")\n",
    "\n",
    "# 3. Total acres by fallback zoning bucket (upzoned if available)\n",
    "acres_by_upzoned_zone = (\n",
    "    other_gdf.groupby(\"zoning_bucket_with_upzoning\")[\"acreage\"].sum().reset_index()\n",
    ")\n",
    "acres_by_upzoned_zone.columns = [\"zoning_bucket\", \"total_acres_with_upzoning\"]\n",
    "\n",
    "# 4. Merge for comparison (optional)\n",
    "comparison = pd.merge(\n",
    "    acres_by_original_zone, acres_by_upzoned_zone, on=\"zoning_bucket\", how=\"outer\"\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[\"diff\"] = comparison[\"total_acres_with_upzoning\"] - comparison[\"total_acres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include zones with net acreage gain\n",
    "comparison_positive = comparison[comparison[\"diff\"] > 0].copy()\n",
    "\n",
    "# Add tax_per_acre_recent from zone_summary\n",
    "comparison_positive[\"tax_per_acre_recent\"] = comparison_positive[\"zoning_bucket\"].map(\n",
    "    lambda z: zone_summary.get(z, {}).get(\"tax_per_acre_recent\", 0)\n",
    ")\n",
    "\n",
    "# Calculate estimated new tax from upzoning\n",
    "comparison_positive[\"estimated_tax_increase\"] = (\n",
    "    comparison_positive[\"diff\"] * comparison_positive[\"tax_per_acre_recent\"]\n",
    ")\n",
    "\n",
    "# Sum total tax increase\n",
    "total_tax_increase = comparison_positive[\"estimated_tax_increase\"].sum() * 0.2478\n",
    "\n",
    "print(\n",
    "    f\"💰 Total estimated tax increase from net upzoning acreage: ${total_tax_increase:,.0f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[\"total_acres_with_upzoning\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[\"total_acres\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Estimate new tax and difference\n",
    "def estimate_new_tax(row):\n",
    "    new_zone = row[\"upzoned\"]\n",
    "    if pd.notnull(new_zone) and new_zone in zone_summary:\n",
    "        return zone_summary[new_zone][\"tax_per_acre_recent\"] * row[\"acreage\"]\n",
    "    return row[\"Property_Tax_Value\"]\n",
    "\n",
    "\n",
    "other_gdf[\"estimated_upzoned_tax\"] = other_gdf.apply(estimate_new_tax, axis=1)\n",
    "other_gdf[\"estimated_tax_increase\"] = (\n",
    "    other_gdf[\"estimated_upzoned_tax\"] - other_gdf[\"Property_Tax_Value\"]\n",
    ")\n",
    "\n",
    "# STEP 5: Optional — Summarize total gain\n",
    "tax_gain_summary = (\n",
    "    other_gdf[other_gdf[\"upzoned\"].notnull()]\n",
    "    .groupby(\"upzoned\")[\"estimated_tax_increase\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(tax_gain_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate tax increase with best performing regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hardcoded_best_models(df, trim_outliers_flag=False):\n",
    "    \"\"\"\n",
    "    Build models based on the hardcoded best configurations from the original analysis\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"\\nBuilding best prediction models for each zoning bucket based on original analysis...\"\n",
    "    )\n",
    "\n",
    "    # Hardcoded best model configurations from the original analysis results\n",
    "    best_model_configs = [\n",
    "        {\n",
    "            \"zone\": \"R1\",\n",
    "            \"feature_set\": \"acreage_latlon\",\n",
    "            \"model_type\": \"Random Forest\",\n",
    "            \"r2_score\": 0.6840,\n",
    "            \"cv_r2_score\": 0.1949,\n",
    "            \"rmse\": 12566.23,\n",
    "        },\n",
    "        {\n",
    "            \"zone\": \"R2\",\n",
    "            \"feature_set\": \"acreage_latlon\",\n",
    "            \"model_type\": \"Random Forest\",\n",
    "            \"r2_score\": 0.6390,\n",
    "            \"cv_r2_score\": -0.0920,\n",
    "            \"rmse\": 5336.64,\n",
    "        },\n",
    "        {\n",
    "            \"zone\": \"R3\",\n",
    "            \"feature_set\": \"acreage_age_zipcode\",\n",
    "            \"model_type\": \"Random Forest\",\n",
    "            \"r2_score\": 0.6643,\n",
    "            \"cv_r2_score\": 0.2030,\n",
    "            \"rmse\": 14791.45,\n",
    "        },\n",
    "        {\n",
    "            \"zone\": \"R4\",\n",
    "            \"feature_set\": \"acreage_age_latlon\",\n",
    "            \"model_type\": \"Random Forest\",\n",
    "            \"r2_score\": 0.5918,\n",
    "            \"cv_r2_score\": 0.3867,\n",
    "            \"rmse\": 40605.66,\n",
    "        },\n",
    "        # {\n",
    "        #     \"zone\": \"R5\",\n",
    "        #     \"feature_set\": \"acreage_age_latlon\",\n",
    "        #     \"model_type\": \"Random Forest\",\n",
    "        #     \"r2_score\": 0.2033,\n",
    "        #     \"cv_r2_score\": -0.0849,\n",
    "        #     \"rmse\": 18870.68,\n",
    "        # },\n",
    "        {\n",
    "            \"zone\": \"R5\",\n",
    "            \"feature_set\": \"acreage_age_latlon\",\n",
    "            \"model_type\": \"Random Forest\",\n",
    "            \"r2_score\": 0.7350,\n",
    "            \"cv_r2_score\": -1.1099,\n",
    "            \"rmse\": 1123487.58,\n",
    "        },\n",
    "        {\n",
    "            \"zone\": \"Parking\",\n",
    "            \"feature_set\": \"acreage_age_latlon\",\n",
    "            \"model_type\": \"Linear Regression\",\n",
    "            \"r2_score\": 0.3204,\n",
    "            \"cv_r2_score\": 0.2190,\n",
    "            \"rmse\": 11800.93,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Now train models using these hardcoded configurations\n",
    "    best_zone_models = {}\n",
    "\n",
    "    for config in best_model_configs:\n",
    "        zone = config[\"zone\"]\n",
    "        model_type = config[\"model_type\"]\n",
    "        feature_set = config[\"feature_set\"]\n",
    "        r2_score_val = config[\"r2_score\"]\n",
    "        cv_r2_score_val = config[\"cv_r2_score\"]\n",
    "        rmse_val = config[\"rmse\"]\n",
    "\n",
    "        try:\n",
    "            # Print information\n",
    "            print(f\"\\nZONE: {zone}\")\n",
    "            print(f\"Best feature set: {feature_set}\")\n",
    "            print(f\"Best model: {model_type}\")\n",
    "            print(f\"R² Score: {r2_score_val:.4f}\")\n",
    "            print(f\"CV R² Score: {cv_r2_score_val:.4f}\")\n",
    "            print(f\"RMSE: {rmse_val:.2f}\")\n",
    "\n",
    "            # Determine features based on the best feature set\n",
    "            use_age = \"_age\" in feature_set\n",
    "            min_year = 0 if use_age else 2015\n",
    "\n",
    "            # Prepare data for this zone with appropriate features\n",
    "            df_processed = prepare_data(\n",
    "                df,\n",
    "                min_year=min_year,\n",
    "                trim_outliers_flag=trim_outliers_flag,\n",
    "                use_age=use_age,\n",
    "            )\n",
    "\n",
    "            # Filter for just this zone\n",
    "            zone_data = df_processed[\n",
    "                df_processed[\"assigned_zoning_bucket\"] == zone\n",
    "            ].copy()\n",
    "\n",
    "            if len(zone_data) < 10:\n",
    "                print(\n",
    "                    f\"Skipping zone {zone} - insufficient data (only {len(zone_data)} samples)\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Set up features based on the best feature set\n",
    "            base_features = [\"acreage\"]\n",
    "            if use_age:\n",
    "                base_features.append(\"Parcel_age\")\n",
    "            if \"latlon\" in feature_set:\n",
    "                base_features.extend([\"Location Latitude\", \"Location Longitude\"])\n",
    "\n",
    "            # Add zip code features if needed\n",
    "            feature_columns = base_features.copy()\n",
    "            if \"zipcode\" in feature_set:\n",
    "                # Add the one-hot encoding for zip codes\n",
    "                if \"zip_code\" in zone_data.columns:\n",
    "                    zip_dummies = pd.get_dummies(zone_data[\"zip_code\"], prefix=\"zip\")\n",
    "                    zone_data = pd.concat([zone_data, zip_dummies], axis=1)\n",
    "                    zip_columns = [\n",
    "                        col for col in zone_data.columns if col.startswith(\"zip_\")\n",
    "                    ]\n",
    "                    feature_columns.extend(zip_columns)\n",
    "\n",
    "            # Prepare X and y\n",
    "            X = zone_data[feature_columns].values\n",
    "            y = zone_data[\"Property_Tax_Value\"].values\n",
    "\n",
    "            # Instantiate the best model type\n",
    "            if model_type == \"Linear Regression\":\n",
    "                model = LinearRegression()\n",
    "            elif model_type == \"Ridge Regression\":\n",
    "                model = Ridge(alpha=1.0)\n",
    "            elif model_type == \"Lasso Regression\":\n",
    "                model = Lasso(alpha=0.1)\n",
    "            elif model_type == \"ElasticNet\":\n",
    "                model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "            elif model_type == \"SVR\":\n",
    "                model = Pipeline(\n",
    "                    [(\"scaler\", StandardScaler()), (\"svr\", LinearSVR(max_iter=10000))]\n",
    "                )\n",
    "            elif model_type == \"Random Forest\":\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=50, random_state=42, n_jobs=4\n",
    "                )\n",
    "            elif model_type == \"Gradient Boosting\":\n",
    "                model = GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
    "            else:\n",
    "                print(f\"Unknown model type {model_type} for zone {zone}\")\n",
    "                continue\n",
    "\n",
    "            # Train the model on the full dataset for this zone\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Store all zip_columns for this zone's model\n",
    "            all_zip_columns = []\n",
    "            if \"zipcode\" in feature_set:\n",
    "                all_zip_columns = [\n",
    "                    col for col in zone_data.columns if col.startswith(\"zip_\")\n",
    "                ]\n",
    "\n",
    "            # Store the trained model with its configuration\n",
    "            best_zone_models[zone] = {\n",
    "                \"model\": model,\n",
    "                \"feature_set\": feature_set,\n",
    "                \"features\": feature_columns,\n",
    "                \"use_age\": use_age,\n",
    "                \"uses_zipcode\": \"zipcode\" in feature_set,\n",
    "                \"zip_columns\": all_zip_columns,  # Store all possible zip columns\n",
    "                \"r2_score\": r2_score_val,\n",
    "                \"cv_r2_score\": cv_r2_score_val,\n",
    "                \"rmse\": rmse_val,\n",
    "            }\n",
    "            print(f\"Successfully trained model for zone {zone}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing zone {zone}: {str(e)}\")\n",
    "\n",
    "    print(best_zone_models)\n",
    "    return best_zone_models\n",
    "\n",
    "\n",
    "# def predict_property_tax_with_models(row, best_zone_models, new_building=True):\n",
    "#     \"\"\"\n",
    "#     Predict property tax using the best model for the target zoning bucket.\n",
    "#     If new_building is True, age will be set to 0 for models using age.\n",
    "#     Only applies if 'upzoned' is a real zone different from the original.\n",
    "#     \"\"\"\n",
    "#     target_zone = row[\"upzoned\"]\n",
    "\n",
    "#     # Skip prediction if not an actual upzone or zone model doesn't exist\n",
    "#     if pd.isnull(target_zone) or target_zone not in best_zone_models:\n",
    "#         return row[\"Property_Tax_Value\"]\n",
    "\n",
    "#     # Get model config for the upzoned zone\n",
    "#     model_config = best_zone_models[target_zone]\n",
    "#     model = model_config[\"model\"]\n",
    "#     feature_set = model_config[\"feature_set\"]\n",
    "#     use_age = model_config[\"use_age\"]\n",
    "#     uses_zipcode = model_config[\"uses_zipcode\"]\n",
    "#     zip_columns = model_config.get(\"zip_columns\", [])\n",
    "\n",
    "#     # Construct feature vector\n",
    "#     feature_values = [row[\"acreage\"]]\n",
    "\n",
    "#     if use_age:\n",
    "#         age = 0 if new_building else max(0, 2025 - row.get(\"Effective Year\", 2025))\n",
    "#         feature_values.append(age)\n",
    "\n",
    "#     if \"latlon\" in feature_set:\n",
    "#         feature_values.append(row[\"Location Latitude\"])\n",
    "#         feature_values.append(row[\"Location Longitude\"])\n",
    "\n",
    "#     if uses_zipcode:\n",
    "#         zip_code = row.get(\"zip_code\")\n",
    "#         for col in zip_columns:\n",
    "#             col_zip = col.split(\"_\")[1]\n",
    "#             feature_values.append(1 if str(zip_code) == col_zip else 0)\n",
    "\n",
    "#     try:\n",
    "#         X_predict = np.array(feature_values).reshape(1, -1)\n",
    "#         if X_predict.shape[1] != len(model_config[\"features\"]):\n",
    "#             print(f\"Feature mismatch for zone {target_zone}\")\n",
    "#             return row[\"Property_Tax_Value\"]\n",
    "#         predicted_tax = model.predict(X_predict)[0]\n",
    "#         return max(0, predicted_tax)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Prediction error for zone {target_zone}: {str(e)}\")\n",
    "#         return row[\"Property_Tax_Value\"]\n",
    "\n",
    "\n",
    "def predict_property_tax_with_models(row, best_zone_models, new_building=True):\n",
    "    \"\"\"\n",
    "    Predict property tax using the best model for the target zoning bucket\n",
    "    If new_building is True, age will be set to 0 for models using age\n",
    "    \"\"\"\n",
    "    target_zone = row[\"upzoned\"]\n",
    "\n",
    "    # If no upzoning or the target zone doesn't have a model, return current tax value\n",
    "    if pd.isnull(target_zone) or target_zone not in best_zone_models:\n",
    "        return row[\"Property_Tax_Value\"]\n",
    "\n",
    "    # Get the best model and its configuration for the target zone\n",
    "    model_config = best_zone_models[target_zone]\n",
    "    model = model_config[\"model\"]\n",
    "    feature_set = model_config[\"feature_set\"]\n",
    "    use_age = model_config[\"use_age\"]\n",
    "    uses_zipcode = model_config[\"uses_zipcode\"]\n",
    "    zip_columns = model_config.get(\"zip_columns\", [])\n",
    "\n",
    "    # Build feature vector in the correct order\n",
    "    feature_values = []\n",
    "\n",
    "    # Always add acreage\n",
    "    feature_values.append(row[\"acreage\"])\n",
    "\n",
    "    # Add age if used\n",
    "    if use_age:\n",
    "        if new_building:\n",
    "            feature_values.append(0)  # New building has age = 0\n",
    "        else:\n",
    "            current_year = 2025\n",
    "            effective_year = (\n",
    "                row[\"Effective Year\"]\n",
    "                if pd.notna(row[\"Effective Year\"])\n",
    "                else current_year\n",
    "            )\n",
    "            feature_values.append(current_year - effective_year)\n",
    "\n",
    "    # Add lat/lon if used\n",
    "    if \"latlon\" in feature_set:\n",
    "        feature_values.append(row[\"Location Latitude\"])\n",
    "        feature_values.append(row[\"Location Longitude\"])\n",
    "\n",
    "    # Handle zip code features if needed\n",
    "    if uses_zipcode:\n",
    "        # Get the zip code for this row\n",
    "        zip_code = (\n",
    "            row[\"zip_code\"] if \"zip_code\" in row and pd.notna(row[\"zip_code\"]) else None\n",
    "        )\n",
    "\n",
    "        # Create one-hot encoding for this zip code\n",
    "        for col in zip_columns:\n",
    "            # Extract the zip from the column name (format: \"zip_XXXXX\")\n",
    "            col_zip = col.split(\"_\")[1]\n",
    "            feature_values.append(\n",
    "                1 if zip_code is not None and col_zip == str(zip_code) else 0\n",
    "            )\n",
    "\n",
    "    # Make prediction\n",
    "    try:\n",
    "        # Reshape for single prediction\n",
    "        X_predict = np.array(feature_values).reshape(1, -1)\n",
    "\n",
    "        # Check if feature count matches model expectation\n",
    "        expected_features = len(model_config[\"features\"])\n",
    "        if X_predict.shape[1] != expected_features:\n",
    "            print(\n",
    "                f\"Feature mismatch for zone {target_zone}: expected {expected_features}, got {X_predict.shape[1]}\"\n",
    "            )\n",
    "            return row[\"Property_Tax_Value\"]\n",
    "\n",
    "        # Predict tax value\n",
    "        predicted_tax = model.predict(X_predict)[0]\n",
    "\n",
    "        # Ensure predicted value is not negative\n",
    "        return max(0, predicted_tax)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting tax for zone {target_zone}: {str(e)}\")\n",
    "        # Fall back to current tax value if prediction fails\n",
    "        return row[\"Property_Tax_Value\"]\n",
    "\n",
    "\n",
    "def estimate_new_tax_with_models(df, best_zone_models, new_building=True):\n",
    "    \"\"\"\n",
    "    Estimate new tax values for upzoned parcels using the best ML models\n",
    "    \"\"\"\n",
    "    # Apply the prediction function to each row\n",
    "    tqdm.pandas(desc=\"Estimating new tax values\")\n",
    "    df[\"estimated_upzoned_tax_model\"] = df.progress_apply(\n",
    "        lambda row: predict_property_tax_with_models(\n",
    "            row, best_zone_models, new_building\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Calculate tax increase\n",
    "    df[\"estimated_tax_increase_model\"] = (\n",
    "        df[\"estimated_upzoned_tax_model\"] - df[\"Property_Tax_Value\"]\n",
    "    )\n",
    "\n",
    "    # Also run the simple per-acre method for comparison (assuming this function exists)\n",
    "    if \"estimate_new_tax\" in globals():\n",
    "        df[\"estimated_upzoned_tax_per_acre\"] = df.progress_apply(\n",
    "            estimate_new_tax, axis=1\n",
    "        )\n",
    "        df[\"estimated_tax_increase_per_acre\"] = (\n",
    "            df[\"estimated_upzoned_tax_per_acre\"] - df[\"Property_Tax_Value\"]\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_estimation_methods(df):\n",
    "    \"\"\"\n",
    "    Compare the ML model estimation results with the simple per-acre method\n",
    "    \"\"\"\n",
    "    # Only consider rows with upzoning\n",
    "    upzoned_df = df[df[\"upzoned\"].notnull()].copy()\n",
    "\n",
    "    if len(upzoned_df) == 0:\n",
    "        print(\"No upzoned parcels to compare\")\n",
    "        return\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    total_current_tax = upzoned_df[\"Property_Tax_Value\"].sum()\n",
    "    total_model_tax = upzoned_df[\"estimated_upzoned_tax_model\"].sum()\n",
    "\n",
    "    model_increase = total_model_tax - total_current_tax\n",
    "\n",
    "    print(\"\\n=== TAX ESTIMATION COMPARISON ===\")\n",
    "    print(f\"Total Current Tax: ${total_current_tax:,.2f}\")\n",
    "    print(f\"Total Estimated Tax (ML Models): ${total_model_tax:,.2f}\")\n",
    "\n",
    "    # Only include per-acre method if it was calculated\n",
    "    if \"estimated_upzoned_tax_per_acre\" in upzoned_df.columns:\n",
    "        total_per_acre_tax = upzoned_df[\"estimated_upzoned_tax_per_acre\"].sum()\n",
    "        per_acre_increase = total_per_acre_tax - total_current_tax\n",
    "        print(f\"Total Estimated Tax (Per Acre): ${total_per_acre_tax:,.2f}\")\n",
    "        print(\n",
    "            f\"Total Tax Increase (Per Acre): ${per_acre_increase:,.2f} ({per_acre_increase / total_current_tax:.1%})\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Total Tax Increase (ML Models): ${model_increase:,.2f} ({model_increase / total_current_tax:.1%})\"\n",
    "    )\n",
    "\n",
    "    # Compare by zone\n",
    "    print(\"\\n=== TAX INCREASE BY ZONE ===\")\n",
    "\n",
    "    # Group by target upzoned zone and calculate total increase for each method\n",
    "    agg_dict = {\"Property_Tax_Value\": \"sum\", \"estimated_tax_increase_model\": \"sum\"}\n",
    "\n",
    "    if \"estimated_tax_increase_per_acre\" in upzoned_df.columns:\n",
    "        agg_dict[\"estimated_tax_increase_per_acre\"] = \"sum\"\n",
    "\n",
    "    zone_comparison = upzoned_df.groupby(\"upzoned\").agg(agg_dict).reset_index()\n",
    "\n",
    "    # Calculate percentage increases\n",
    "    zone_comparison[\"model_increase_pct\"] = (\n",
    "        zone_comparison[\"estimated_tax_increase_model\"]\n",
    "        / zone_comparison[\"Property_Tax_Value\"]\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    if \"estimated_tax_increase_per_acre\" in zone_comparison.columns:\n",
    "        zone_comparison[\"per_acre_increase_pct\"] = (\n",
    "            zone_comparison[\"estimated_tax_increase_per_acre\"]\n",
    "            / zone_comparison[\"Property_Tax_Value\"]\n",
    "            * 100\n",
    "        )\n",
    "\n",
    "    # Sort by ML model tax increase\n",
    "    zone_comparison = zone_comparison.sort_values(\n",
    "        \"estimated_tax_increase_model\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    for _, row in zone_comparison.iterrows():\n",
    "        zone = row[\"upzoned\"]\n",
    "        current = row[\"Property_Tax_Value\"]\n",
    "        model_inc = row[\"estimated_tax_increase_model\"]\n",
    "        model_pct = row[\"model_increase_pct\"]\n",
    "\n",
    "        print(f\"\\nZone: {zone}\")\n",
    "        print(f\" Current Tax: ${current:,.2f}\")\n",
    "        print(f\" ML Model Increase: ${model_inc:,.2f} ({model_pct:.1f}%)\")\n",
    "\n",
    "        if \"estimated_tax_increase_per_acre\" in row:\n",
    "            acre_inc = row[\"estimated_tax_increase_per_acre\"]\n",
    "            acre_pct = row[\"per_acre_increase_pct\"]\n",
    "            print(f\" Per Acre Increase: ${acre_inc:,.2f} ({acre_pct:.1f}%)\")\n",
    "\n",
    "    # Create visualization comparing methods\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Sort by zone name for better readability\n",
    "    plot_df = zone_comparison.sort_values(\"upzoned\")\n",
    "    x = range(len(plot_df))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(\n",
    "        [i - width / 2 for i in x],\n",
    "        plot_df[\"estimated_tax_increase_model\"],\n",
    "        width=width,\n",
    "        label=\"ML Model Estimate\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    if \"estimated_tax_increase_per_acre\" in plot_df.columns:\n",
    "        plt.bar(\n",
    "            [i + width / 2 for i in x],\n",
    "            plot_df[\"estimated_tax_increase_per_acre\"],\n",
    "            width=width,\n",
    "            label=\"Per Acre Estimate\",\n",
    "            color=\"green\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Target Zoning Bucket\")\n",
    "    plt.ylabel(\"Estimated Tax Increase ($)\")\n",
    "    plt.title(\"Comparison of Tax Increase Estimation Methods by Zone\")\n",
    "    plt.xticks(x, plot_df[\"upzoned\"], rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"tax_increase_method_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Create percentage comparison chart\n",
    "    if \"per_acre_increase_pct\" in plot_df.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(\n",
    "            [i - width / 2 for i in x],\n",
    "            plot_df[\"model_increase_pct\"],\n",
    "            width=width,\n",
    "            label=\"ML Model Estimate (%)\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        plt.bar(\n",
    "            [i + width / 2 for i in x],\n",
    "            plot_df[\"per_acre_increase_pct\"],\n",
    "            width=width,\n",
    "            label=\"Per Acre Estimate (%)\",\n",
    "            color=\"green\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        plt.xlabel(\"Target Zoning Bucket\")\n",
    "        plt.ylabel(\"Percentage Tax Increase (%)\")\n",
    "        plt.title(\"Comparison of Percentage Tax Increase by Zone\")\n",
    "        plt.xticks(x, plot_df[\"upzoned\"], rotation=45, ha=\"right\")\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(\n",
    "        #     \"tax_increase_percentage_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        # )\n",
    "        plt.close()\n",
    "\n",
    "    return zone_comparison\n",
    "\n",
    "\n",
    "# Updated main function using hardcoded best models\n",
    "def main_with_hardcoded_models(df):\n",
    "    \"\"\"\n",
    "    Main function that uses hardcoded best models from the original analysis\n",
    "    \"\"\"\n",
    "    # Build models using the hardcoded best configurations\n",
    "    best_zone_models = build_hardcoded_best_models(df, trim_outliers_flag=False)\n",
    "    print(\"estimating now\")\n",
    "    # Estimate new taxes using the trained models\n",
    "    df_with_estimates = estimate_new_tax_with_models(df, best_zone_models)\n",
    "\n",
    "    # Compare the estimation methods\n",
    "    zone_comparison = compare_estimation_methods(df_with_estimates)\n",
    "\n",
    "    print(\"\\nAnalysis complete! Check the output files for detailed results.\")\n",
    "\n",
    "    return df_with_estimates, zone_comparison, best_zone_models\n",
    "\n",
    "\n",
    "# To execute this updated code:\n",
    "df_with_estimates, zone_comparison, best_zone_models = main_with_hardcoded_models(\n",
    "    other_gdf\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
